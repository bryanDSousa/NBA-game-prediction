{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de funciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def GS(a,b):\n",
    "    \"\"\"\"\"\n",
    "    Función que recibe dos parámetros;\n",
    "    :a: una variable binaria que representa 0 = bueno y 1 = malo (objetivo)\n",
    "    :b: predicción de la primera variable (continua, entera o binaria)\n",
    "    :return: coeficiente GINI de las dos variables anteriores. \"\"\"\n",
    "    \n",
    "    gini = 2*roc_auc_score(a,b)-1\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de entranamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(x_train, y_train, x_test, y_test, method):  \n",
    "    \"\"\"\n",
    "    Funcion para entrenar un modelo con el método seleccionado\n",
    "    El entrenamiento y algoritmos son de la libreria de sklearn.\n",
    "    :param x_train: numpy array, required\n",
    "    :param y_train: numpy array, required\n",
    "    :param x_test: numpy array, required\n",
    "    :param y_test: numpy array, required    \n",
    "    :return: object\n",
    "        - Modelo entrenado según los datos\n",
    "    \"\"\"    \n",
    "    if method == 'LR':  # Linear Regresssion\n",
    "        return LR(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'LOGR': # Logistic Regresssion\n",
    "        return LOGR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'DT': # Decision Tree Classifier\n",
    "        return DT(x_train, y_train, x_test, y_test)    \n",
    "    \n",
    "    elif method == 'LASSO': # Lasso Regresssion \n",
    "        return LASSO(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RIDGE': # Ridge Regresssion\n",
    "        return RIDGE(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RFR': # Random Forest Regressor\n",
    "        return RFR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'RFC': # Random Forest Classifier\n",
    "        return RFC(x_train, y_train, x_test, y_test)    \n",
    "\n",
    "    elif method == 'GBR': # Gradient Boosting Regression\n",
    "        return GBR(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones resumen de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bryan.de.sousa\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def LR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Linear Regresssion\n",
    "    \"\"\"\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "def LOGR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Logistic Regresssion\n",
    "    \"\"\"\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DT(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(random_state=99).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "def LASSO(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Lasso Regresssion\n",
    "    \"\"\"\n",
    "    model = Lasso(alpha = 0.01).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "def RIDGE(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Ridge Regresssion\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha = 0.01).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def RFR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Regressor\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RFC(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Classifier\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "def GBR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regression\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(n_estimators=1000,alpha=0.01).fit(X_train, y_train)\n",
    "    return dictionary_of_measures(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_of_measures(model, X_train, y_train, X_test, y_test):\n",
    "    # MEDIDAS DE PRUEBA\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad.\n",
    "        y_pred_train = model.predict_proba(X_train)[:,1] #seleccionamos solo la columna de prob. igual a 1\n",
    "    except:\n",
    "        y_pred_train = model.predict(X_test)\n",
    "        \n",
    "    a_train = model.score(X_train, y_train)\n",
    "    gini_train = GS(y_train,y_pred_train)    \n",
    "\n",
    "    # MEDIDAS DE TEST\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad\n",
    "        y_pred_test = model.predict_proba(X_test)[:,1] #seleccionamos solo la columna de prob. igual a 1\n",
    "    except:\n",
    "        y_pred_test = model.predict(X_test) \n",
    "        \n",
    "    a_test = model.score(X_test, y_test)\n",
    "    gini_test = GS(y_test,y_pred_test)    \n",
    "\n",
    "    return {'model':model,'accuracy_train':a_train,'accuracy_test':a_test,\n",
    "            'gini_train':gini_train,'gini_test':gini_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos de Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = \"\"\"{\"svm_linear\": SVC(probability=True, kernel='linear', C=1.0),\n",
    "                       \"svm_poly\": SVC(probability=True, kernel='poly', C=1.0),\n",
    "                       \"svm_rbf\": SVC(probability=True, kernel='rbf', C=1.0, gamma=0.01),\n",
    "                       \"linear_svc\": LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.1, C=1.0, multi_class='ovr', fit_intercept=True,\n",
    "                                               intercept_scaling=1, random_state=None, max_iter=3000),\n",
    "                       \"knn\": KNeighborsClassifier(n_neighbors=100, weights='distance', leaf_size=30, n_jobs=n_jobs),\n",
    "                       \"random_forests\": RandomForestClassifier(n_estimators=350, criterion='entropy', min_samples_split=2,\n",
    "                                                                min_samples_leaf=1, max_leaf_nodes=600, n_jobs=n_jobs),\n",
    "                       \"logistic_regression\": LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=2.4, fit_intercept=True, intercept_scaling=1,\n",
    "                                                                 random_state=None, solver='liblinear', max_iter=1000, multi_class='ovr',\n",
    "                                                                 warm_start=False, n_jobs=n_jobs),\n",
    "                       \"decision_trees\": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2,\n",
    "                                                                min_samples_leaf=100, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                                                random_state=None, max_leaf_nodes=None, presort=False),\n",
    "                       \"sgd\": SGDClassifier(alpha=.0001, n_iter=500, penalty=\"elasticnet\", n_jobs=n_jobs),\n",
    "                       \"neural_network\": Classifier(layers=[Layer(\"Sigmoid\", units=14), Layer(\"Sigmoid\", units=13), Layer(\"Sigmoid\", units=12),\n",
    "                                                            Layer(\"Sigmoid\", units=10), Layer(\"Softmax\")], learning_rate=0.01, n_iter=200,\n",
    "                                                    batch_size=10, regularize='L1', n_stable=50, dropout_rate=0, verbose=True),\n",
    "                       \"GBC\": GradientBoostingClassifier(max_depth=10, max_leaf_nodes=850, min_samples_leaf=15, learning_rate=0.1),\n",
    "                       \"XGB\": XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "                                            max_depth=10, min_child_weight=2, missing=None, n_estimators=100, nthread=n_jobs, reg_alpha=0,\n",
    "                                            objective='binary:logistic', reg_lambda=1, scale_pos_weight=1, seed=0, silent=True, subsample=1)}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_curves_regressor(estimator, data, features, target, train_sizes, cv):\n",
    "    \n",
    "    train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    estimator, data[features], data[target], train_sizes =\n",
    "    train_sizes,\n",
    "    cv = cv, scoring = 'neg_mean_squared_error')\n",
    "    train_scores_mean = -train_scores.mean(axis = 1)\n",
    "    validation_scores_mean = -validation_scores.mean(axis = 1)\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, label = 'Training error')\n",
    "    plt.plot(train_sizes, validation_scores_mean, label = 'Validation error')\n",
    "\n",
    "    plt.ylabel('MSE', fontsize = 14)\n",
    "    plt.xlabel('Training set size', fontsize = 14)\n",
    "    title = 'Learning curves for a ' + str(estimator).split('(')[0] + ' model'\n",
    "    plt.title(title, fontsize = 18, y = 1.03)\n",
    "    plt.legend()\n",
    "    plt.ylim(0,40)\n",
    "\n",
    "### Plotting the two learning curves ###\n",
    "\n",
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#plt.figure(figsize = (16,5))\n",
    "\n",
    "#for model, i in [(RandomForestRegressor(), 1), (LinearRegression(),2)]:\n",
    "    #plt.subplot(1,2,i)\n",
    "    #learning_curves(model, electricity, features, target, train_sizes, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor de Inflación de la Varianza (VIF)\n",
    "\n",
    "Utilizaremos esta función para el descarte de variables perfectamente colineales.\n",
    "\n",
    "Vamos a calcular el valor del VIF para todas las variables menos la objetivo. Para esto se realiza una regresión lineal de cada una de las variables frente al resto y aplicamos la fórmula del VIF\n",
    "\n",
    "\n",
    "$$\n",
    "    VIF_i = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "\n",
    "El valor del VIF se encuentra acotado ente 1 (no existe multicolinealidad) e infinito (existe una multicolinealidad perfecta). \n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Al ser una regresión simple, el coeficiente de determinacion (R-square) es simplemente el cuadrado del coeficiente de correlación de Pearson.\n",
    "\n",
    "En este contexto, mientras más cercano a 1 mejor, porque quiere decir que menos correlación tiene una variable respecto a la otra.\n",
    "\n",
    "\n",
    "Excluimos las variables que tengan VIF mayor a 5 porque son aquellas que tienen un R-square mayor a 0,8\n",
    "\n",
    "**La salida de esta funcion es una lista de variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculateVIF(data):\n",
    "    features = list(data.columns)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    result = pd.DataFrame(index = ['VIF'], columns = features)\n",
    "    result = result.fillna(0) # por las dudas\n",
    "    \n",
    "    for ite in range(num_features):\n",
    "        x_features = features[:]\n",
    "        y_feature = features[ite]\n",
    "        x_features.remove(y_feature)\n",
    "        \n",
    "        x = data[x_features]\n",
    "        y = data[y_feature]\n",
    "        \n",
    "        model.fit(x,y)\n",
    "        \n",
    "        if model.score(x,y) == 1:\n",
    "            result[y_feature] = Infinity\n",
    "        else:\n",
    "            result[y_feature] = 1/(1 - model.score(x,y))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "#calculateVIF(partidos.iloc[:, :-1]) # Excluimos la variale de estudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectDataUsingVIF(data, max_VIF):\n",
    "    \n",
    "    result = data.copy(deep = True)\n",
    "    \n",
    "    VIF = calculateVIF(result)\n",
    "    \n",
    "    while VIF.values.max() > max_VIF:\n",
    "        col_max = np.where(VIF == VIF.values.max())[1][0]\n",
    "        features = list(result.columns)\n",
    "        features.remove(features[col_max])\n",
    "        print('Se ha eliminado: ----- '+ str(features[col_max]) + \" ----- VIF:  \" + \n",
    "              str(VIF[features[col_max]].values))\n",
    "        \n",
    "        result = result[features]\n",
    "        \n",
    "        VIF = calculateVIF(result)\n",
    "        \n",
    "    return result\n",
    "\n",
    "#variables_vif = list(calculateVIF(selectDataUsingVIF(partidos.iloc[:, :-1], 10)).columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables: Algoritmo Genético (GA)\n",
    "### Código comentado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from deap import creator, base, tools, algorithms # LIBRERIA DE ALGORITMO GENÉTICO - instalar (Anaconda): pip install deap\n",
    "import random\n",
    "from sklearn import metrics\n",
    "\n",
    "list_inputs = set(x_train.columns) # colocar la lista de variables de entrenamiento\n",
    "\n",
    "\n",
    "# CONFIGURACIÓN DEL ALGORITMO GENÉTICO. \n",
    "# STARTING POOL (STARTING CANDIDATE POPULATION)\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(list_inputs))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "def evalOneMax(individual):\n",
    "    return sum(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "NPOPSIZE = len(x_train.columns) #RANDOM STARTING POOL SIZE\n",
    "population = toolbox.population(n=NPOPSIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "#####\n",
    "# EVALUACIÓN DE GINI PARA EL NPOPSIZE CREADO\n",
    "#####\n",
    "dic_gini={}\n",
    "\n",
    "for i in range(np.shape(population)[0]): \n",
    "\n",
    "    # LISTA DE VARIABLES\n",
    "    var_model = []    \n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if (population[i])[j]==1:\n",
    "            var_model.append(list(list_inputs)[j])\n",
    "\n",
    "    # EVALUACIÓN DEL INDICE DE GINI PARA CADA VARIABLE EN LA STARTING POOL \n",
    "    \n",
    "    X_train = x_train.copy() # copiamos para no modificar los datos\n",
    "    Y_train = y_train.copy()\n",
    "\n",
    "    ######\n",
    "    # EVALUACIÓN DEL MODELO PARA EL CALCULO DEL ALGORITMO GENÉTICO\n",
    "    #####      \n",
    "\n",
    "    RFC = RandomForestClassifier()\n",
    "    model = RFC.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict_proba(X_train)[:,1]\n",
    "            \n",
    "    ######\n",
    "    # INICIO: DESARROLLO DE LA MÉTRICA DE EVALUACIÓN DEL MODELO PARA SELECCIONAR VARIABLES (GINI)\n",
    "    ######\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict) # Es posible cambiar la métrica en este punto\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    gini_power= 2*roc_auc_score(Y_train, Y_predict)-1\n",
    "    \n",
    "    #gini_power = abs(2*auc-1)\n",
    "    ######\n",
    "    # FINAL: DESARROLLO DE LA MÉTRICA DE EVALUACIÓN DEL MODELO PARA SELECCIONAR VARIABLES (GINI)\n",
    "    #####                \n",
    "    \n",
    "    gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "    dic_gini[gini]=population[j] \n",
    "    \n",
    "list_gini = sorted(dic_gini.keys(),reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "#####\n",
    "# INICIO: LOOP DEL ALGORITMO GENÉTICO\n",
    "####\n",
    "# Se itera las veces que sea necesario hasta que no mejore la métrica de evaluación del modelo\n",
    "# de esta forma es posible encontrar el óptimo de variables para conseguir el máximo gini posible.\n",
    "#####\n",
    "sum_current_gini = 0.0\n",
    "sum_current_gini_1 = 0.0\n",
    "sum_current_gini_2 = 0.0\n",
    "first = 0    \n",
    "OK = 1\n",
    "a = 0\n",
    "while OK:  # Se repite hasta que no mejore el modelo, al menos un poco. Para GINI en 2 generaciones\n",
    "    a = a + 1\n",
    "    print('loop ', a)\n",
    "    OK=0\n",
    "\n",
    "    ####\n",
    "    # INICIO: OFFSPRING\n",
    "    ####\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1) #CROSS-X PROBABILITY = 50%, MUTATION PROBABILITY=10%\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population =toolbox.select(offspring, k=len(population))\n",
    "    ####\n",
    "    # FINAL: OFFSPRING\n",
    "    ####\n",
    "\n",
    "    sum_current_gini_2 = sum_current_gini_1\n",
    "    sum_current_gini_1 = sum_current_gini\n",
    "    sum_current_gini = 0.0\n",
    "\n",
    "    #####\n",
    "    #INICIO: EVALUACION DE GINI EN EL OFFSPRING (DESCENDENCIA)\n",
    "    #####\n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if population[j] not in dic_gini.values(): \n",
    "            var_model = [] \n",
    "            for i in range(np.shape(population)[0]): \n",
    "                if (population[j])[i]==1:\n",
    "                    var_model.append(list(list_inputs)[i])\n",
    "            \n",
    "            X_train= x_train.copy()\n",
    "            Y_train= y_train.copy()\n",
    "            \n",
    "            ######\n",
    "            # EVALUACIÓN DEL OFFSPRING (DESCENDENCIA) INICIAL\n",
    "            #####    \n",
    "    \n",
    "            \n",
    "            RFC = RandomForestClassifier()\n",
    "            model = RFC.fit(X_train, Y_train)\n",
    "            Y_predict = model.predict_proba(X_train)[:,1]                      \n",
    "            \n",
    "            ######\n",
    "            # INICIO: DESARROLLO DE LA MÉTRICA DE EVALUACIÓN DEL MODELO PARA SELECCIONAR VARIABLES (GINI)\n",
    "            #####                       \n",
    "            fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict)\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            gini_power= 2*roc_auc_score(Y_train, Y_predict)-1   \n",
    "            \n",
    "            #gini_power = abs(2*auc-1)\n",
    "            ######\n",
    "            # FINAL: DESARROLLO DE LA MÉTRICA DE EVALUACIÓN DEL MODELO PARA SELECCIONAR VARIABLES (GINI)\n",
    "            #####                       \n",
    "           \n",
    "            gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "            dic_gini[gini]=population[j]  \n",
    "    #####\n",
    "    # FINAL: EVALUACION DE GINI EN EL OFFSPRING (DESCENDENCIA)\n",
    "    #####\n",
    "\n",
    "    #####\n",
    "    # INICIO: SELECCIÓN DE LAS MEJORES VARIABLES\n",
    "    #####           \n",
    "    list_gini=sorted(dic_gini.keys(),reverse=True)\n",
    "    population=[]\n",
    "    for i in list_gini[:NPOPSIZE]:\n",
    "        population.append(dic_gini[i])\n",
    "        gini=float(i.split(';')[0])\n",
    "        sum_current_gini+=gini\n",
    "    #####\n",
    "    # FINAL: SELECCIÓN DE LAS MEJORES VARIABLES\n",
    "    #####           \n",
    "      \n",
    "    # MEJORA DEL GINI EN LAS ÚLTIMAS DOS GENERACIONES\n",
    "    print ('sum_current_gini=', sum_current_gini)\n",
    "    if(sum_current_gini>sum_current_gini_1+0.0001 or sum_current_gini>sum_current_gini_2+0.0001):\n",
    "        OK=1\n",
    "#####\n",
    "# FINAL: LOOP DEL ALGORITMO GENÉTICO\n",
    "#####\n",
    "\n",
    "\n",
    "gini_max=list_gini[0]        \n",
    "gini=float(gini_max.split(';')[0])\n",
    "features=gini_max.split(';')[1]\n",
    "\n",
    "\n",
    "####\n",
    "# LISTA DE VARIABLES SELECCIONADAS\n",
    "#####\n",
    "#adn_variables = []\n",
    "#f=0\n",
    "#for i in range(len(features)):\n",
    "#    if features[i]=='1':\n",
    "#        f+=1\n",
    "#        adn_variables.append(list(list_inputs)[i])\n",
    "#        print ('feature ', f, ':', list(list_inputs)[i])\n",
    "#print ('gini: ', gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
