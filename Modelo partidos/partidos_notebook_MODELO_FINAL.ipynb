{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario de términos\n",
    "\n",
    "MP -- Minutes Played\n",
    "\n",
    "FG -- Field Goals\n",
    "\n",
    "FGA -- Field Goal Attempts\n",
    "\n",
    "FG% -- Field Goal Percentage\n",
    "\n",
    "3P -- 3-Point Field Goals\n",
    "\n",
    "3PA -- 3-Point Field Goal Attempts\n",
    "\n",
    "3P% -- 3-Point Field Goal Percentage\n",
    "\n",
    "FT -- Free Throws\n",
    "\n",
    "FTA -- Free Throw Attempts\n",
    "\n",
    "FT% -- Free Throw Percentage\n",
    "\n",
    "ORB -- Offensive Rebounds\n",
    "\n",
    "DRB -- Defensive Rebounds\n",
    "\n",
    "TRB -- Total Rebounds\n",
    "\n",
    "AST -- Assists\n",
    "\n",
    "STL -- Steals\n",
    "\n",
    "BLK -- Blocks\n",
    "\n",
    "TOV -- Turnovers\n",
    "\n",
    "PF -- Personal Fouls\n",
    "\n",
    "PTS -- Points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OFF RATE: **offensive rating**  is a statistic used to measure either a teams offensive performance or an individual player's efficiency at producing points.\n",
    "\n",
    "**Defensive Player Rating** = (Players Points.Total FG%) + **Opponents Differential**= 1/5 of possessions - Times Fouled+ FTM* FT% * OAPOW( Official Adjusted Players Offensive Withstand). \n",
    "\n",
    "***********************************************************************************************\n",
    "\n",
    "DEFF RATE: **Defensive rating** is a statistic used to measure an individual player's efficiency at preventing the other team from scoring points\n",
    "\n",
    "**Defensive Player Rating** = (Players Steals.Blocks) + **Opponents Differential**= 1/5 of possessions - Times blown by + Deflections * OAPDW( Official Adjusted Players Defensive Withstand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "%pylab\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte I: Exploración, limpieza, selección de variables, entre otros.\n",
    "## Analisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>local_team</th>\n",
       "      <th>visitor_team</th>\n",
       "      <th>Points</th>\n",
       "      <th>Opponent_Points</th>\n",
       "      <th>Result</th>\n",
       "      <th>LOCAL_Racha</th>\n",
       "      <th>...</th>\n",
       "      <th>local_played_visitor</th>\n",
       "      <th>visitor_played_visitor</th>\n",
       "      <th>visitor_played_local</th>\n",
       "      <th>place_local_played</th>\n",
       "      <th>place_visitor_played</th>\n",
       "      <th>avg_distance_place_visitor_played</th>\n",
       "      <th>avg_distance_place_local_played</th>\n",
       "      <th>distance_between_stadiums</th>\n",
       "      <th>distance_local_traveled</th>\n",
       "      <th>distance_visitor_traveled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Nov 7</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>97</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Milwaukee Bucks</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>190.000000</td>\n",
       "      <td>329.333333</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>988.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nov 9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>88</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Nada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nov 11</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>103</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Nov 16</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Dallas Mavericks</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>314.0</td>\n",
       "      <td>7682.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Dec 18</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>97</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>-8</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Atlanta Hawks</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>203.500000</td>\n",
       "      <td>226.000000</td>\n",
       "      <td>407.0</td>\n",
       "      <td>452.0</td>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Jan 4</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>109</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Sacramento Kings</td>\n",
       "      <td>Minnesota Timberwolves</td>\n",
       "      <td>694.500000</td>\n",
       "      <td>600.400000</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>3002.0</td>\n",
       "      <td>1389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Jan 7</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Atlanta Hawks</td>\n",
       "      <td>98</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>203.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Jan 10</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>85</td>\n",
       "      <td>95</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Washington Wizards</td>\n",
       "      <td>148.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Jan 14</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>111</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Washington Wizards</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Jan 26</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Phoenix Suns</td>\n",
       "      <td>113</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>Phoenix Suns</td>\n",
       "      <td>144.285714</td>\n",
       "      <td>301.000000</td>\n",
       "      <td>1010.0</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>1010.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Feb 3</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Atlanta Hawks</td>\n",
       "      <td>86</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>770.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>452.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Feb 6</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>103</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>-4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Indiana Pacers</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>990.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>Feb 8</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Los Angeles Clippers</td>\n",
       "      <td>92</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Nada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4006.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Feb 23</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>115</td>\n",
       "      <td>124</td>\n",
       "      <td>1</td>\n",
       "      <td>-4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Dallas Mavericks</td>\n",
       "      <td>Nada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Feb 26</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Washington Wizards</td>\n",
       "      <td>94</td>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>Washington Wizards</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>313.500000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>594.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Mar 9</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>104</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "      <td>-12</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Miami Heat</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>81.428571</td>\n",
       "      <td>770.000000</td>\n",
       "      <td>570.0</td>\n",
       "      <td>2310.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Mar 11</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>95</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>-13</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>138.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>415.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>415.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Mar 17</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Washington Wizards</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Brooklyn Nets</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>207.500000</td>\n",
       "      <td>594.0</td>\n",
       "      <td>415.0</td>\n",
       "      <td>2964.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Mar 18</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Oklahoma City Thunder</td>\n",
       "      <td>97</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>-3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2086.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Apr 8</td>\n",
       "      <td>2016</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>102</td>\n",
       "      <td>109</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Nada</td>\n",
       "      <td>Indiana Pacers</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>407.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>990.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0    Date  Year  Season          local_team  \\\n",
       "0            0   Nov 7  2015    2016  Philadelphia 76ers   \n",
       "1            1   Nov 9  2015    2016  Philadelphia 76ers   \n",
       "2            2  Nov 11  2015    2016  Philadelphia 76ers   \n",
       "3            3  Nov 16  2015    2016  Philadelphia 76ers   \n",
       "4            4  Dec 18  2015    2016  Philadelphia 76ers   \n",
       "5            5   Jan 4  2016    2016  Philadelphia 76ers   \n",
       "6            6   Jan 7  2016    2016  Philadelphia 76ers   \n",
       "7            7  Jan 10  2016    2016  Philadelphia 76ers   \n",
       "8            8  Jan 14  2016    2016  Philadelphia 76ers   \n",
       "9            9  Jan 26  2016    2016  Philadelphia 76ers   \n",
       "10          10   Feb 3  2016    2016  Philadelphia 76ers   \n",
       "11          11   Feb 6  2016    2016  Philadelphia 76ers   \n",
       "12          12   Feb 8  2016    2016  Philadelphia 76ers   \n",
       "13          13  Feb 23  2016    2016  Philadelphia 76ers   \n",
       "14          14  Feb 26  2016    2016  Philadelphia 76ers   \n",
       "15          15   Mar 9  2016    2016  Philadelphia 76ers   \n",
       "16          16  Mar 11  2016    2016  Philadelphia 76ers   \n",
       "17          17  Mar 17  2016    2016  Philadelphia 76ers   \n",
       "18          18  Mar 18  2016    2016  Philadelphia 76ers   \n",
       "19          19   Apr 8  2016    2016  Philadelphia 76ers   \n",
       "\n",
       "              visitor_team  Points  Opponent_Points  Result  LOCAL_Racha  ...  \\\n",
       "0            Orlando Magic      97              105       1           -5  ...   \n",
       "1            Chicago Bulls      88              111       1           -6  ...   \n",
       "2          Toronto Raptors     103              119       1           -7  ...   \n",
       "3         Dallas Mavericks      86               92       1          -10  ...   \n",
       "4          New York Knicks      97              107       1           -8  ...   \n",
       "5   Minnesota Timberwolves     109               99       0           -2  ...   \n",
       "6            Atlanta Hawks      98              126       1            1  ...   \n",
       "7      Cleveland Cavaliers      85               95       1           -2  ...   \n",
       "8            Chicago Bulls     111              115       1           -3  ...   \n",
       "9             Phoenix Suns     113              103       0           -1  ...   \n",
       "10           Atlanta Hawks      86              124       1           -2  ...   \n",
       "11           Brooklyn Nets     103               98       0           -4  ...   \n",
       "12    Los Angeles Clippers      92               98       1            1  ...   \n",
       "13           Orlando Magic     115              124       1           -4  ...   \n",
       "14      Washington Wizards      94              103       1           -6  ...   \n",
       "15         Houston Rockets     104              118       1          -12  ...   \n",
       "16           Brooklyn Nets      95               89       0          -13  ...   \n",
       "17      Washington Wizards      94               99       1           -2  ...   \n",
       "18   Oklahoma City Thunder      97              111       1           -3  ...   \n",
       "19         New York Knicks     102              109       1            1  ...   \n",
       "\n",
       "   local_played_visitor  visitor_played_visitor  visitor_played_local  \\\n",
       "0                     1                       1                     0   \n",
       "1                     0                       0                     0   \n",
       "2                     0                       0                     1   \n",
       "3                     1                       1                     0   \n",
       "4                     1                       0                     1   \n",
       "5                     1                       0                     1   \n",
       "6                     0                       1                     0   \n",
       "7                     0                       1                     0   \n",
       "8                     0                       1                     0   \n",
       "9                     1                       0                     1   \n",
       "10                    0                       1                     0   \n",
       "11                    0                       1                     0   \n",
       "12                    0                       0                     0   \n",
       "13                    1                       0                     0   \n",
       "14                    1                       0                     1   \n",
       "15                    1                       0                     1   \n",
       "16                    0                       0                     1   \n",
       "17                    1                       1                     0   \n",
       "18                    0                       1                     0   \n",
       "19                    0                       1                     0   \n",
       "\n",
       "    place_local_played    place_visitor_played  \\\n",
       "0      Milwaukee Bucks         Houston Rockets   \n",
       "1   Philadelphia 76ers                    Nada   \n",
       "2   Philadelphia 76ers         Toronto Raptors   \n",
       "3    San Antonio Spurs         Houston Rockets   \n",
       "4        Atlanta Hawks         New York Knicks   \n",
       "5     Sacramento Kings  Minnesota Timberwolves   \n",
       "6   Philadelphia 76ers         New York Knicks   \n",
       "7   Philadelphia 76ers      Washington Wizards   \n",
       "8   Philadelphia 76ers      Washington Wizards   \n",
       "9        Orlando Magic            Phoenix Suns   \n",
       "10  Philadelphia 76ers              Miami Heat   \n",
       "11  Philadelphia 76ers          Indiana Pacers   \n",
       "12  Philadelphia 76ers                    Nada   \n",
       "13    Dallas Mavericks                    Nada   \n",
       "14     Detroit Pistons      Washington Wizards   \n",
       "15          Miami Heat         Houston Rockets   \n",
       "16  Philadelphia 76ers           Brooklyn Nets   \n",
       "17       Brooklyn Nets               Utah Jazz   \n",
       "18  Philadelphia 76ers          Boston Celtics   \n",
       "19                Nada          Indiana Pacers   \n",
       "\n",
       "    avg_distance_place_visitor_played  avg_distance_place_local_played  \\\n",
       "0                          190.000000                       329.333333   \n",
       "1                                 NaN                         0.000000   \n",
       "2                          301.000000                         0.000000   \n",
       "3                          285.000000                      3841.000000   \n",
       "4                          203.500000                       226.000000   \n",
       "5                          694.500000                       600.400000   \n",
       "6                          203.500000                         0.000000   \n",
       "7                          148.500000                         0.000000   \n",
       "8                          198.000000                         0.000000   \n",
       "9                          144.285714                       301.000000   \n",
       "10                         770.000000                         0.000000   \n",
       "11                         330.000000                         0.000000   \n",
       "12                                NaN                         0.000000   \n",
       "13                                NaN                       157.000000   \n",
       "14                          99.000000                       313.500000   \n",
       "15                          81.428571                       770.000000   \n",
       "16                         138.333333                         0.000000   \n",
       "17                         494.000000                       207.500000   \n",
       "18                         822.000000                         0.000000   \n",
       "19                         198.000000                              NaN   \n",
       "\n",
       "    distance_between_stadiums  distance_local_traveled  \\\n",
       "0                      1806.0                    988.0   \n",
       "1                      1002.0                      0.0   \n",
       "2                       301.0                      0.0   \n",
       "3                       314.0                   7682.0   \n",
       "4                       407.0                    452.0   \n",
       "5                      1389.0                   3002.0   \n",
       "6                       452.0                      0.0   \n",
       "7                      1464.0                      0.0   \n",
       "8                      1002.0                      0.0   \n",
       "9                      1010.0                   1806.0   \n",
       "10                      452.0                      0.0   \n",
       "11                      415.0                      0.0   \n",
       "12                     4006.0                      0.0   \n",
       "13                     1806.0                    314.0   \n",
       "14                      594.0                    627.0   \n",
       "15                      570.0                   2310.0   \n",
       "16                      415.0                      0.0   \n",
       "17                      594.0                    415.0   \n",
       "18                     2086.0                      0.0   \n",
       "19                      407.0                      NaN   \n",
       "\n",
       "    distance_visitor_traveled  \n",
       "0                       570.0  \n",
       "1                         NaN  \n",
       "2                       301.0  \n",
       "3                       570.0  \n",
       "4                       407.0  \n",
       "5                      1389.0  \n",
       "6                       407.0  \n",
       "7                       594.0  \n",
       "8                       594.0  \n",
       "9                      1010.0  \n",
       "10                     2310.0  \n",
       "11                      990.0  \n",
       "12                        NaN  \n",
       "13                        NaN  \n",
       "14                      594.0  \n",
       "15                      570.0  \n",
       "16                      415.0  \n",
       "17                     2964.0  \n",
       "18                     1644.0  \n",
       "19                      990.0  \n",
       "\n",
       "[20 rows x 116 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Nuevas_variables/v5/input_extendidov5.csv')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3248"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2739"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "Date\n",
      "Year\n",
      "Season\n",
      "local_team\n",
      "visitor_team\n",
      "Points\n",
      "Opponent_Points\n",
      "Result\n",
      "LOCAL_Racha\n",
      "ID Partido\n",
      "LOCAL_Ultimos10Victorias\n",
      "LOCAL_Ultimos10VDerrotas\n",
      "VISITANTE_Ultimos10Victorias\n",
      "VISITANTE_Ultimos10VDerrotas\n",
      "VISITANTE_Racha\n",
      "local_Conf_position\n",
      "local_Win\n",
      "local_Lose\n",
      "local_Percentagewl\n",
      "local_Dif_leader\n",
      "local_Home_win\n",
      "local_Home_lose\n",
      "local_Away_win\n",
      "local_Away_lose\n",
      "local_Div_win\n",
      "local_Div_lose\n",
      "local_Cnf_win\n",
      "local_Cnf_lose\n",
      "local_Icf_win\n",
      "local_Icf_lose\n",
      "visitor_Conf_position\n",
      "visitor_Win\n",
      "visitor_Lose\n",
      "visitor_Percentagewl\n",
      "visitor_Dif_leader\n",
      "visitor_Home_win\n",
      "visitor_Home_lose\n",
      "visitor_Away_win\n",
      "visitor_Away_lose\n",
      "visitor_Div_win\n",
      "visitor_Div_lose\n",
      "visitor_Cnf_win\n",
      "visitor_Cnf_lose\n",
      "visitor_Icf_win\n",
      "visitor_Icf_lose\n",
      "local_fg\n",
      "local_fga\n",
      "local_fg3\n",
      "local_fg3a\n",
      "local_ft\n",
      "local_fta\n",
      "local_orb\n",
      "local_drb\n",
      "local_trb\n",
      "local_ast\n",
      "local_stl\n",
      "local_blk\n",
      "local_tov\n",
      "local_pf\n",
      "local_pts\n",
      "local_ft_pct\n",
      "local_fg_pct\n",
      "local_fg3_pct\n",
      "visitor_fg\n",
      "visitor_fga\n",
      "visitor_fg3\n",
      "visitor_fg3a\n",
      "visitor_ft\n",
      "visitor_fta\n",
      "visitor_orb\n",
      "visitor_drb\n",
      "visitor_trb\n",
      "visitor_ast\n",
      "visitor_stl\n",
      "visitor_blk\n",
      "visitor_tov\n",
      "visitor_pf\n",
      "visitor_pts\n",
      "visitor_ft_pct\n",
      "visitor_fg_pct\n",
      "visitor_fg3_pct\n",
      "Sueldo local\n",
      "Sueldo visitante\n",
      "Local_Division\n",
      "Local_Conferencia\n",
      "Visitor_Division\n",
      "Visitor_Conferencia\n",
      "LOCAL_AWS_MEDIO_AGRUPADO\n",
      "VISITOR_AWS_MEDIO_AGRUPADO\n",
      "number_date\n",
      "local_dif_between_previous_game\n",
      "visitor_dif_between_previous_game\n",
      "local_played_previous_date\n",
      "visitor_played_previous_date\n",
      "local_played_two_days_ago\n",
      "visitor_played_two_days_ago\n",
      "local_played_three_days_ago\n",
      "visitor_played_three_days_ago\n",
      "local_played_prorrogue_previous_date\n",
      "visitor_played_prorrogue_previous_date\n",
      "local_played_prorrogue_two_days_ago\n",
      "visitor_played_prorrogue_two_days_ago\n",
      "local_played_prorrogue_three_days_ago\n",
      "visitor_played_prorrogue_three_days_ago\n",
      "local_played_local\n",
      "local_played_visitor\n",
      "visitor_played_visitor\n",
      "visitor_played_local\n",
      "place_local_played\n",
      "place_visitor_played\n",
      "avg_distance_place_visitor_played\n",
      "avg_distance_place_local_played\n",
      "distance_between_stadiums\n",
      "distance_local_traveled\n",
      "distance_visitor_traveled\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de variables compuestas\n",
    "\n",
    "Indicar en el estudio la seleccion de variables por logica del modelo, y comentar aquellas observadas en estudios previos que no tienen ningun sentido incluir\n",
    "\n",
    "Por ejemplo la ilogica inclusion de la diferencia_visitante_point observado en estudios previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMENTAR A VICTOR ARREGLAR EL NOMBRE DE LAS VARIABLES\n",
    "\n",
    "data.rename(columns={'LOCAL_Ultimos10VDerrotas':'LOCAL_Ultimos10Derrotas',\n",
    "                    'VISITANTE_Ultimos10VDerrotas': 'VISITANTE_Ultimos10Derrotas',\n",
    "                    'Result': 'target'}, inplace=True)\n",
    "\n",
    "\n",
    "data['local_visitor_dif_pts'] = data['local_pts'] - data['visitor_pts']\n",
    "\n",
    "data['LOCAL_porcentaje_victoria_Ultimos10'] = (data['LOCAL_Ultimos10Victorias']) / (data['LOCAL_Ultimos10Victorias'] + data['LOCAL_Ultimos10Derrotas'])\n",
    "data['VISITANTE_porcentaje_victoria_Ultimos10'] = (data['VISITANTE_Ultimos10Victorias']) / (data['VISITANTE_Ultimos10Victorias'] + data['VISITANTE_Ultimos10Derrotas'])\n",
    "\n",
    "# COMENTAR A VICTOR SI ES POSIBLE SACAR ESTAS VARIABLES, PREVIAMENTE SON MAS IMPORTANTES QUE \n",
    "# LOCAL_porcentajeVictorias O local_Percentagewl\n",
    "\n",
    "data['LOCAL_porcentaje_victoria_LOCAL'] = (data['local_Home_win']) / (data['local_Home_win'] + data['local_Home_lose'])\n",
    "data['VISITANTE_porcentaje_victoria_VISITANTE'] = (data['visitor_Home_win']) / (data['visitor_Home_win'] + data['visitor_Home_lose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aqui pasa algo raro con los denominadores de las variables que hace INDETERMINADO los calculos y lleva luego a NA\n",
    "\n",
    "data = data.dropna()\n",
    "len(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion de variables categoricas\n",
    "\n",
    "Estrictamente necesario transformar las variables categoricas para modelar con las librerias de Python que estamos utilizando.\n",
    "\n",
    "Distintos enfoques para trabajar variables categoricas: https://pbpython.com/categorical-encoding.html\n",
    "\n",
    "Trabajar con variables categoricas en Machine Learning: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "\n",
    "## Investigar mas a fondo sobre:\n",
    "## \"Maldición de la dimensionalidad\"\n",
    "\n",
    "¿Un posible problema por la escasez de datos y la alta dimensionalidad del modelo? Cerca de 100 variables previo a selección (pendiente de profundizar) \n",
    "\n",
    "https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['Local_Conferencia', 'Local_Division'],\n",
    "               prefix=['Conf_local', 'Div_local'])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Visitor_Conferencia', 'Visitor_Division'],\n",
    "               prefix=['Conf_visitor', 'Div_visitor'])\n",
    "\n",
    "data.rename(columns={'Div_local_Atlantic Division':'Div_local_Atlantic',\n",
    "                    'Div_local_Central Division': 'Div_local_Central',\n",
    "                    'Div_local_Northwest Division': 'Div_local_Northwest',\n",
    "                    'Div_local_Pacific Division': 'Div_local_Pacific',\n",
    "                    'Div_local_Southeast Division': 'Div_local_Southeast',\n",
    "                    'Div_local_Southwest Division': 'Div_local_Southwest'},inplace=True)\n",
    "\n",
    "data.rename(columns={'Div_visitor_Atlantic Division':'Div_visitor_Atlantic',\n",
    "                    'Div_visitor_Central Division': 'Div_visitor_Central',\n",
    "                    'Div_visitor_Northwest Division': 'Div_visitor_Northwest',\n",
    "                    'Div_visitor_Pacific Division': 'Div_visitor_Pacific',\n",
    "                    'Div_visitor_Southeast Division': 'Div_visitor_Southeast',\n",
    "                    'Div_visitor_Southwest Division': 'Div_visitor_Southwest'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccion de variables para análisis de correlación\n",
    "\n",
    "local = []\n",
    "def filter_local_name(name):\n",
    "    if 'Local' in name:\n",
    "      return name\n",
    "    if 'LOCAL' in name:\n",
    "        return name\n",
    "    if 'local' in name:\n",
    "        return name\n",
    "    \n",
    "for variable in data.columns:\n",
    "    if (filter_local_name(variable) != None):\n",
    "        local.append(variable)\n",
    "print(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO LO VEO. Como mucho si se quiere se puede calcular si en los ultimos 10 partidos se enfrentaron estos equipos\n",
    "\n",
    "data['place_local_played'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de Variables por Matriz de Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copiamos y pegamos el output del código anterior, incluyendo ahora \"target\"\n",
    "\n",
    "game_var_local = ['target', 'local_pts', 'local_visitor_dif_pts', 'local_fg', 'local_fga', 'local_fg3', \n",
    "                  'local_fg3a', 'local_ft', 'local_fta', 'local_orb', 'local_drb', 'local_trb', 'local_ast',\n",
    "                  'local_stl', 'local_blk', 'local_tov','local_pf','local_ft_pct', 'local_fg_pct', 'local_fg3_pct']\n",
    "\n",
    "game_var_visitante = ['target', 'visitor_pts', 'local_visitor_dif_pts', 'visitor_fg', 'visitor_fga', \n",
    "                    'visitor_fg3', 'visitor_fg3a', 'visitor_ft', 'visitor_fta', 'visitor_orb', 'visitor_drb',\n",
    "                    'visitor_trb', 'visitor_ast','visitor_stl', 'visitor_blk', 'visitor_tov','visitor_pf',\n",
    "                    'visitor_ft_pct', 'visitor_fg_pct', 'visitor_fg3_pct']\n",
    "\n",
    "\n",
    "prev_var_local = ['target', 'LOCAL_Racha', 'LOCAL_Ultimos10Victorias', 'LOCAL_Ultimos10Derrotas',\n",
    "                  'LOCAL_porcentaje_victoria_Ultimos10', 'local_Home_win', 'local_Home_lose', 'local_Percentagewl',\n",
    "                  'LOCAL_porcentaje_victoria_LOCAL','local_Conf_position','local_Dif_leader',\n",
    "                  'Conf_local_Este', 'Div_local_Atlantic', 'Div_local_Central',\n",
    "                  'Div_local_Northwest', 'Div_local_Pacific', \n",
    "                  'Div_local_Southeast', 'Div_local_Southwest', 'Sueldo local',\n",
    "                  'local_dif_between_previous_game', 'local_played_previous_date', 'local_played_local', \n",
    "                  'local_played_visitor', 'avg_distance_place_local_played', 'distance_local_traveled',\n",
    "                  'LOCAL_AWS_MEDIO_AGRUPADO']\n",
    "\n",
    "\n",
    "prev_var_visitante = ['target', 'VISITANTE_Racha', 'VISITANTE_Ultimos10Victorias', 'VISITANTE_Ultimos10Derrotas',\n",
    "                      'VISITANTE_porcentaje_victoria_Ultimos10', 'visitor_Home_win', 'visitor_Home_lose',\n",
    "                      'visitor_Percentagewl','VISITANTE_porcentaje_victoria_VISITANTE','visitor_Conf_position',\n",
    "                      'visitor_Dif_leader','Conf_visitor_Este', 'Div_visitor_Atlantic', 'Div_visitor_Central',\n",
    "                      'Div_visitor_Northwest', 'Div_visitor_Pacific', \n",
    "                      'Div_visitor_Southeast', 'Div_visitor_Southwest', 'Sueldo visitante',\n",
    "                      'visitor_dif_between_previous_game', 'visitor_played_previous_date', 'visitor_played_visitor', \n",
    "                      'visitor_played_local', 'avg_distance_place_visitor_played', 'distance_visitor_traveled']\n",
    "\n",
    "\n",
    "\n",
    "prev_data_local = data[prev_var_local]\n",
    "prev_data_visitante = data[prev_var_visitante]\n",
    "\n",
    "game_data_local = data[game_var_local]\n",
    "game_data_visitante = data[game_var_visitante]\n",
    "\n",
    "all_variables = game_var_local + game_var_visitante + prev_var_local + prev_var_visitante\n",
    "len(all_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables = list(dict.fromkeys(all_variables)) #drop duplicates\n",
    "del all_variables[0] #borramos target de la primera posicion\n",
    "\n",
    "all_variables.append('target') #insertamos target en la ultima posicion\n",
    "\n",
    "print('Total variables:  '+ str(len(all_variables)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “Garbage in, garbage out”\n",
    "\n",
    "Hemos eliminado las siguientes variables previo al análisis, se sabe a priori que dado el estudio no enriquecen el modelo en ninguna manera (LOCAL Y VISITANTE):\n",
    "\n",
    "**'local_team', 'local_Win', 'local_Lose', 'local_Away_win', 'local_Away_lose', 'local_Div_win', 'local_Div_lose', 'local_Cnf_win', 'local_Cnf_lose', 'local_Icf_win', 'local_Icf_lose'**\n",
    "\n",
    "**'Conf_local_Oeste'** por defecto tiene colinealidad perfecta con 'Conf_local_Este'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_data_local.hist(figsize = (20, 20))\n",
    "matplotlib.axes.Axes.remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_data_local.hist(figsize = (20, 20))\n",
    "matplotlib.axes.Axes.remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap PREV LOCAL\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns = sns.heatmap(prev_data_local.corr(), annot=True, fmt='.2f')\n",
    "sns.set_title('Matriz de Correlación Previo al Partido LOCAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap PREV VISITANTE\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_2 = sns.heatmap(prev_data_visitante.corr(), annot=True, fmt='.2f')\n",
    "sns_2.set_title('Matriz de Correlación Previo al Partido VISITANTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap GAME LOCAL\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_3 = sns.heatmap(game_data_local.corr(), annot=True, fmt='.2f')\n",
    "sns_3.set_title('Matriz de Correlación Durante el Partido LOCAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap GAME VISITANTE\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_3 = sns.heatmap(game_data_visitante.corr(), annot=True, fmt='.2f')\n",
    "sns_3.set_title('Matriz de Correlación Durante el Partido VISITANTE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de Matriz de Correlación\n",
    "\n",
    "### Importante:\n",
    "\n",
    "El descarte de variables por escasez de correlación o por multicolinealidad solo tiene sentido si se va a plantear un modelo sustentado en estimaciones lineales, de lo contrario la exclusión de estas contribuye al deteriodo del funcionamiento de otros modelos, por ejemplo, cualquiera derivado de \"árboles de decisión\".\n",
    "\n",
    "El principal argumento es que la medida de dependencia entre variables no necesariamente es lineal, y de ser lineal, los algoritmos basados en árboles de decisión dividen entre sus funciones lógicas de decisión las variables que son perfectamente correlacionadas.\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Sobre la variable **target** descartamos las siguientes variables:\n",
    "\n",
    "**LOCAL_porcentajeVictorias** y escogemos LOCAL_porcentaje_victoria_LOCAL dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.86)\n",
    "\n",
    "**LOCAL_Posicion_division** y escogemos LOCAL_Posicion dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.83)\n",
    "\n",
    "**LOCAL_Posicion_conferencia** y escogemos LOCAL_Posicion dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.97)\n",
    "\n",
    "**Conf_local_west** y escogemos Conf_local_east por la inversa correlación perfecta que existe entre estas. No tiene sentido contemplar ambas ya que una contiene la información de la otra cuando toma valor cero.\n",
    "\n",
    "Si existe la posibilidad de descartar otra variable porque pueda generar ruido en el modelo sería **LOCAL_Racha** pero dejemos esta variable mientras tanto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos = data[all_variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partidos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables por Factor de Inflación de la Varianza (VIF)\n",
    "\n",
    "La existencia de multicolinealidad --explicación de variables independientes entre si mediante una combinación lineal de otras variables independientes-- es un problema grave en cualquier método estadístico que tenga como objetivo la predicción de una variable. Una de las técnicas que se puede utilizar para identificar este problema es el VIF.\n",
    "\n",
    "**El objetivo principal es eliminar aquellas variables que se explican entre si.**\n",
    "\n",
    "Vamos a calcular el valor del VIF para todas las variables menos la objetivo. Para esto se realiza una regresión lineal de cada una de las variables frente al resto y aplicamos la fórmula del VIF\n",
    "$$\n",
    "    VIF_i = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "El valor del VIF se encuentra acotado ente 1 (no existe multicolinealidad) e infinito (existe una multicolinealidad perfecta). Es habitual eliminar las características con un valor por encima de 5, aunque dependiendo del número de características se puede relajar este criterio.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Esta funcion no es mas que la iteración de una regresión lineal de cada variable respecto a otra.\n",
    "\n",
    "Al ser siempre una regresión simple, el coeficiente de determinacion (R-square) es simplemente el cuadrado del coeficiente de correlación de Pearson.\n",
    "\n",
    "En este contexto (para lo que nos interesa) mientras más cercano a 1 mejor, porque quiere decir que menos correlación tiene una variable respecto a la otra.\n",
    "\n",
    "\n",
    "Excluimos las variables que tengan VIF mayor a 5 porque son aquellas que tienen un R-square mayor a 0,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculateVIF(data):\n",
    "    features = list(data.columns)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    result = pd.DataFrame(index = ['VIF'], columns = features)\n",
    "    result = result.fillna(0)\n",
    "    \n",
    "    for ite in range(num_features):\n",
    "        x_features = features[:]\n",
    "        y_featue = features[ite]\n",
    "        x_features.remove(y_featue)\n",
    "        \n",
    "        x = data[x_features]\n",
    "        y = data[y_featue]\n",
    "        \n",
    "        model.fit(data[x_features], data[y_featue])\n",
    "        \n",
    "        if model.score(data[x_features], data[y_featue]) == 1:\n",
    "            result[y_featue] = Infinity\n",
    "        else:\n",
    "            result[y_featue] = 1/(1 - model.score(data[x_features], data[y_featue]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "calculateVIF(partidos.loc[:,partidos.columns != 'target']) # Excluimos la variale de estuido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la funcion anterior da salida a cada una de las variables con su VIF correspondiente, ahora construimos una funcion para la exclusión de aquellas con VIF mayor a 5. \n",
    "\n",
    "**La salida de esta funcion es una lista de variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectDataUsingVIF(data, max_VIF = 5):\n",
    "    result = data.copy(deep = True)\n",
    "    \n",
    "    VIF = calculateVIF(result)\n",
    "    \n",
    "    while VIF.values.max() > max_VIF:\n",
    "        col_max = np.where(VIF == VIF.values.max())[1][0]\n",
    "        features = list(result.columns)\n",
    "        features.remove(features[col_max])\n",
    "        print('Se ha eliminado: ----- '+ str(features[col_max]) + \" ----- VIF:  \" + \n",
    "              str(VIF[features[col_max]].values))\n",
    "        \n",
    "        result = result[features]\n",
    "        \n",
    "        VIF = calculateVIF(result)\n",
    "        \n",
    "    return result\n",
    "\n",
    "variables_vif = list(calculateVIF(selectDataUsingVIF(partidos.loc[:,partidos.columns != 'target'], 5)).columns)\n",
    "# Evaluar algun cambio en el código ya que se observan valores inferiores a 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajustes de tipos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partidos['fecha'] = pd.to_datetime(partidos['fecha'].astype(str), format='%Y%m%d')\n",
    "partidos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORADA 2017-2018\n",
    "temporada_2017_2018 = data_1_clean[data_1_clean['fecha'] >= '2017-09-01']\n",
    "temporada_2017_2018 = temporada_2017_2018[temporada_2017_2018['fecha'] <= '2018-04-11'].sort_values('fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temporada_2017_2018 = temporada_2017_2018.reset_index(drop=True)\n",
    "\n",
    "\n",
    "temporada_2017_2018.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORADA 2018-2019\n",
    "temporada_2018_2019 = data_1_clean[data_1_clean['fecha'] >= '2018-09-01']\n",
    "temporada_2018_2019 = temporada_2018_2019[temporada_2018_2019['fecha'] <= '2019-04-10'].sort_values('fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temporada_2018_2019 = temporada_2018_2019.reset_index(drop=True)\n",
    "\n",
    "\n",
    "temporada_2018_2019.head(5)\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte II: Selección de modelos.\n",
    "\n",
    "## Entrenamiento y Test inicial\n",
    "\n",
    "#### Logistic Regression\n",
    "#### Random Forest Classifier\n",
    "\n",
    "Antes de comenzar con la creación del modelo es necesario separar la muestra para el entrenamiento y test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el correcto funcionamiento de las funciones creadas en este apartado y que sea posible la comparación de modelos es estrictamente necesario que los datos de test y entrenamientos tengan el mismo tamaño.\n",
    "\n",
    "Sabemos que este no es el mejor enfoque, pero es el más sencillo y rápido que ayudará a determinar cuales son los modelos con mejor poder de predicción en entrenamiento y test.\n",
    "\n",
    "Es importante destacar que el **% de caida** del modelo es ajustable mediante tecnicas de preparacion de los datos, hiperparametrizacion de los modelos y otras tecnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAY QUE ELIMINAR LA SECUENCIA DE LOS DATOS POR FECHA\n",
    "\n",
    "\n",
    "# un train y test del mismo tamaño para hacer la primera labor de investigacion del modelo\n",
    "#variables = partidos.loc[:, partidos.columns != 'fecha'] \n",
    "\n",
    "\n",
    "#variables_2017_2018 = temporada_2017_2018.loc[:,temporada_2017_2018.columns != 'fecha']\n",
    "\n",
    "#variables_2018_2019 = temporada_2018_2019.loc[:,temporada_2018_2019.columns != 'fecha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# variables_2017_2018, target_2017_2018 = temporada_2017_2018.iloc[:, :-1], temporada_2017_2018.iloc[:, -1:]\n",
    "#variables_2018_2019, target_2018_2019 = temporada_2018_2019.iloc[:, :-1], temporada_2018_2019.iloc[:, -1:]\n",
    "\n",
    "variables = partidos.copy()\n",
    "\n",
    "variables, target = variables.iloc[:, :-1], variables.iloc[:, -1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size = 0.5, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRESENCIA DE DATOS DESBALANCEADOS (¿?) INVESTIGAR ALERTA!!!\n",
    "\n",
    "https://towardsdatascience.com/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36\n",
    "\n",
    "\n",
    "En teoria se deberia realizar una operacion de **Oversampling** ¿por que?:\n",
    "\n",
    "Cuando el conjunto de datos no representa todas las clases de datos por igual, el modelo podría ajustarse en exceso a la clase que está más representada en su conjunto de datos (en nuestro caso como es de esperarse mayor wins por parte del local) y pasar por alto la existencia de la clase minoritaria (los loses del local).\n",
    "\n",
    "En este contexto, de forma exagerada, imaginemos que nuestro modelo siempre estima win para local, dado los datos, sobre la totalidad de resultados igual a 1 tendrá un accuracy aprox. igual a 59%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos.target.value_counts() # Esto ya no parece tener sentido YO ESTABA TRABAJANDO CON UN .CSV ANTIGUO........"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo evaluar correctamente un modelo desbalanceado:\n",
    "\n",
    "### Falta investigar\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall \n",
    "\n",
    "Hay que evaluar si se realiza el oversampling o no. De no realizarse, es importante las métricas de evaluación del modelo:\n",
    "\n",
    "#####  colocar formula con latex\n",
    "\n",
    "El **accuracy** puede ser una métrica engañosa para conjuntos de datos desequilibrados. Considere una muestra con 95 valores negativos y 5 positivos. La clasificación de todos los valores como negativos en este caso da un puntaje de precisión de 0.95. Hay muchas métricas que no sufren este problema. \n",
    "\n",
    "Por ejemplo, la precisión equilibrada (bACC) normaliza las predicciones verdaderas positivas y negativas verdaderas por el número de muestras positivas y negativas, respectivamente, y divide su suma entre dos:\n",
    "\n",
    "\n",
    "##### colocar formula con latex\n",
    "\n",
    "Para el ejemplo anterior (95 muestras negativas y 5 positivas), clasificarlas todas como negativas da un puntaje de precisión equilibrado de 0,5 (el puntaje máximo de bACC es uno), que es equivalente al valor esperado de una suposición aleatoria en un conjunto de datos equilibrado. La precisión equilibrada puede servir como una medida de rendimiento general para un modelo, ya sea que las etiquetas verdaderas estén o no desequilibradas en los datos, suponiendo que el costo de FN (falso negativo) sea el mismo que FP (falso positivo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def GS(a,b):\n",
    "    \"\"\"\"\"\n",
    "    Función que recibe dos parámetros;\n",
    "    :a: una variable binaria que representa 0 = bueno y 1 = malo (objetivo)\n",
    "    :b: predicción de la primera variable (continua, entera o binaria)\n",
    "    :return: coeficiente GINI de las dos variables anteriores. \"\"\"\n",
    "    \n",
    "    gini = 2*roc_auc_score(a,b)-1\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de entranamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(x_train, y_train, x_test, y_test, method):  \n",
    "    \"\"\"\n",
    "    Funcion para entrenar un modelo con el método seleccionado\n",
    "    El entrenamiento y algoritmos son de la libreria de sklearn.\n",
    "    :param x_train: numpy array, required\n",
    "    :param y_train: numpy array, required\n",
    "    :param x_test: numpy array, required\n",
    "    :param y_test: numpy array, required    \n",
    "    :return: object\n",
    "        - Modelo entrenado según los datos\n",
    "    \"\"\"    \n",
    "    if method == 'LR':  # Linear Regresssion\n",
    "        return LR(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'LOGR': # Logistic Regresssion\n",
    "        return LOGR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'DT': # Decision Tree Classifier\n",
    "        return DT(x_train, y_train, x_test, y_test)    \n",
    "    \n",
    "    elif method == 'LASSO': # Lasso Regresssion \n",
    "        return LASSO(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RIDGE': # Ridge Regresssion\n",
    "        return RIDGE(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RFR': # Random Forest Regressor\n",
    "        return RFR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'RFC': # Random Forest Classifier\n",
    "        return RFC(x_train, y_train, x_test, y_test)    \n",
    "\n",
    "    elif method == 'GBR': # Gradient Boosting Regression\n",
    "        return GBR(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones resumen de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def LR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Linear Regresssion\n",
    "    \"\"\"\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "def LOGR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Logistic Regresssion\n",
    "    \"\"\"\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DT(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(min_samples_split=20, random_state=99).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "def LASSO(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Lasso Regresssion\n",
    "    \"\"\"\n",
    "    model = Lasso(alpha = 0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "def RIDGE(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Ridge Regresssion\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha = 0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def RFR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Regressor\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RFC(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Classifier\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "def GBR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regression\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(n_estimators=1000,alpha=0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test):\n",
    "    # MEDIDAS DE PRUEBA\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad.\n",
    "        y_pred_train = model.predict_proba(X_train)[:,1]\n",
    "    except:\n",
    "        y_pred_train = model.predict(X_test)\n",
    "        \n",
    "    a_train = model.score(X_train, y_train)\n",
    "    gini_train = GS(y_test,y_pred_train)    \n",
    "\n",
    "    # MEDIDAS DE TEST\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad.\n",
    "        y_pred_test = model.predict_proba(X_test)[:,1]\n",
    "    except:\n",
    "        y_pred_test = model.predict(X_test) \n",
    "        \n",
    "    a_test = model.score(X_test, y_test)\n",
    "    gini_test = GS(y_test,y_pred_test)    \n",
    "\n",
    "    return {'model':model,'accuracy_train':a_train,'accuracy_test':a_test,\n",
    "            'gini_train':gini_train,'gini_test':gini_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de comparación de modelos (Gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_list = ['LR','LASSO', 'DT', 'LOGR','RIDGE','RFR','RFC','GBR']\n",
    "df_result_summary = pd.DataFrame(index=method_list,columns=['GINI-train','GINI-test', 'Caida %'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = []\n",
    "C2 = []\n",
    "C3 = []\n",
    "for method in method_list:\n",
    "    dict_trained_model[method] = train_method(x_train, y_train,x_test,y_test,method)\n",
    "    # Predicciones - REGRESSIONS\n",
    "    if method in ['LR','LASSO','RIDGE','RFR','GBR']: \n",
    "        y_pred_train = dict_trained_model[method]['model'].predict(x_train)\n",
    "        y_pred_test = dict_trained_model[method]['model'].predict(x_test)\n",
    "    else: # Predicciones - CLASSIFIERS \n",
    "        y_pred_train = dict_trained_model[method]['model'].predict_proba(x_train)[:,1]\n",
    "        y_pred_test = dict_trained_model[method]['model'].predict_proba(x_test)[:,1]\n",
    "    # Calculo de GINI\n",
    "    gini_score_train = GS(y_train,y_pred_train)\n",
    "    C1.append(gini_score_train)\n",
    "    gini_score_test = GS(y_test,y_pred_test)\n",
    "    C2.append(gini_score_test)\n",
    "    \n",
    "    diferencia = (1-(gini_score_test/gini_score_train)) * 100\n",
    "    C3.append(diferencia)\n",
    "    \n",
    "    \n",
    "# Adding the results to: df_result_summary\n",
    "df_result_summary ['GINI-train'] = C1\n",
    "df_result_summary ['GINI-test'] = C2\n",
    "df_result_summary ['Caida %'] = C3\n",
    "\n",
    "# Printing out the summary table\n",
    "df_result_summary.sort_values('GINI-test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de comparación de modelos (Accuracy)\n",
    "\n",
    "#### Accuracy (exactitud)\n",
    "La exactitud es una métrica para evaluar modelos de resultados de clasificación. \n",
    "\n",
    "Informalmente, la exactitud es la fracción de predicciones que el modelo realizó correctamente.\n",
    "\n",
    "Formalmente, la exactitud tiene la siguiente definición:\n",
    "\n",
    "##### Colocar formula con Latex\n",
    "\n",
    "Donde VP = Verdaderos positivos, VN = Verdaderos negativos, FP = Falsos positivos y FN = Falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for method in method_list:\n",
    "    dict_trained_model[method] = train_method(x_train, y_train,x_test,y_test,method)\n",
    "#    print (dict_trained_model[method])\n",
    "    data.append([method, dict_trained_model[method]['accuracy_train'], dict_trained_model[method]['accuracy_test']])\n",
    "\n",
    "df_result_summary_2 = pd.DataFrame(data,columns=['method','accuracy_train','accuracy_test']) \n",
    "                \n",
    "df_result_summary_2.sort_values('accuracy_test', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para guardar el modelo en su posterior uso\n",
    "\n",
    "\n",
    "#filename = 'nba_pred_model_RFC_basic_1.sav'\n",
    "#pickle.dump(nombre_del_objeto_modelo, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anexo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = \"\"\"{\"svm_linear\": SVC(probability=True, kernel='linear', C=1.0),\n",
    "                       \"svm_poly\": SVC(probability=True, kernel='poly', C=1.0),\n",
    "                       \"svm_rbf\": SVC(probability=True, kernel='rbf', C=1.0, gamma=0.01),\n",
    "                       \"linear_svc\": LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.1, C=1.0, multi_class='ovr', fit_intercept=True,\n",
    "                                               intercept_scaling=1, random_state=None, max_iter=3000),\n",
    "                       \"knn\": KNeighborsClassifier(n_neighbors=100, weights='distance', leaf_size=30, n_jobs=n_jobs),\n",
    "                       \"random_forests\": RandomForestClassifier(n_estimators=350, criterion='entropy', min_samples_split=2,\n",
    "                                                                min_samples_leaf=1, max_leaf_nodes=600, n_jobs=n_jobs),\n",
    "                       \"logistic_regression\": LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=2.4, fit_intercept=True, intercept_scaling=1,\n",
    "                                                                 random_state=None, solver='liblinear', max_iter=1000, multi_class='ovr',\n",
    "                                                                 warm_start=False, n_jobs=n_jobs),\n",
    "                       \"decision_trees\": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2,\n",
    "                                                                min_samples_leaf=100, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                                                random_state=None, max_leaf_nodes=None, presort=False),\n",
    "                       \"sgd\": SGDClassifier(alpha=.0001, n_iter=500, penalty=\"elasticnet\", n_jobs=n_jobs),\n",
    "                       \"neural_network\": Classifier(layers=[Layer(\"Sigmoid\", units=14), Layer(\"Sigmoid\", units=13), Layer(\"Sigmoid\", units=12),\n",
    "                                                            Layer(\"Sigmoid\", units=10), Layer(\"Softmax\")], learning_rate=0.01, n_iter=200,\n",
    "                                                    batch_size=10, regularize='L1', n_stable=50, dropout_rate=0, verbose=True),\n",
    "                       \"GBC\": GradientBoostingClassifier(max_depth=10, max_leaf_nodes=850, min_samples_leaf=15, learning_rate=0.1),\n",
    "                       \"XGB\": XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "                                            max_depth=10, min_child_weight=2, missing=None, n_estimators=100, nthread=n_jobs, reg_alpha=0,\n",
    "                                            objective='binary:logistic', reg_lambda=1, scale_pos_weight=1, seed=0, silent=True, subsample=1)}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables con RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "list_var = list(set(variables.columns))\n",
    "\n",
    "print (\"FEATURE IMPORTANCE Random Forest Classifier\")\n",
    "print(\"\")\n",
    "\n",
    "       \n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "forest.fit(variables, target) # Este paso es importante: la selección es respecto a la totalidad de los datos\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Ranking\n",
    "for f in range(len(variables.columns)):\n",
    "    print(\"%d. %s (%f) \" % (f + 1, list_var[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_variables = []\n",
    "for f in range(50):\n",
    "    rf_variables.append(list_inputs[indices[f]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train_std = sc.fit_transform(x_train, y_train)\n",
    "y_train_std = sc.transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables con Algoritmo Genético (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from deap import creator, base, tools, algorithms #GENETIC ALGORITHM LIBRARY - requirement: pip install deap\n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"GENETIC ALGORITHM FOR FEATURE SELECTION:\")\n",
    "\n",
    "list_inputs = set(variables.columns)\n",
    "\n",
    "#####\n",
    "#SETING UP THE GENETIC ALGORITHM and CALCULATING STARTING POOL (STARTING CANDIDATE POPULATION)\n",
    "#####\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(list_inputs))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "def evalOneMax(individual):\n",
    "    return sum(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "NPOPSIZE = len(variables.columns) #RANDOM STARTING POOL SIZE\n",
    "population = toolbox.population(n=NPOPSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "#####\n",
    "#ASSESSING GINI ON THE STARTING POOL\n",
    "#####\n",
    "dic_gini={}\n",
    "\n",
    "for i in range(np.shape(population)[0]): \n",
    "\n",
    "    # TRASLATING DNA INTO LIST OF VARIABLES (1-81)\n",
    "    var_model = []    \n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if (population[i])[j]==1:\n",
    "            var_model.append(list(list_inputs)[j])\n",
    "\n",
    "    # ASSESSING GINI INDEX FOR EACH INVIVIDUAL IN THE INITIAL POOL \n",
    "\n",
    "    #X_train = partidos[var_model]\n",
    "    #Y_train = partidos[\"target\"]\n",
    "    \n",
    "    X_train = variables.copy()\n",
    "    Y_train = target.copy()\n",
    "\n",
    "    ######\n",
    "    # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "    #####      \n",
    "\n",
    "    lr = RandomForestClassifier()\n",
    "    model = lr.fit(X_train, Y_train)\n",
    "    Y_predict = result.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    #yo_pred = result.predict_proba(Xo_std)[:,1] #este es para tener la prob. cuando usamos skit.learn    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #lr = sm.Logit(Y_train, X_train)\n",
    "    #model=lr.fit()   \n",
    "    #Y_predict=model.predict(X_train)\n",
    "    ######\n",
    "    # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "    #####             \n",
    "\n",
    "\n",
    "    ######\n",
    "    # CHANGE_HERE - START: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "    #####                \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    gini_power= 2*roc_auc_score(Y_train, Y_predict)-1\n",
    "    #gini_power = abs(2*auc-1)\n",
    "    ######\n",
    "    # CHANGE_HERE - END: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "    #####                \n",
    "    \n",
    "    gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "    dic_gini[gini]=population[j] \n",
    "    \n",
    "list_gini=sorted(dic_gini.keys(),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#GENETIC ALGORITHM MAIN LOOP - START\n",
    "# - ITERATING MANY TIMES UNTIL NO IMPROVMENT HAPPENS IN ORDER TO FIND THE OPTIMAL SET OF CHARACTERISTICS (VARIABLES)\n",
    "#####\n",
    "sum_current_gini=0.0\n",
    "sum_current_gini_1=0.0\n",
    "sum_current_gini_2=0.0\n",
    "first=0    \n",
    "OK = 1\n",
    "a=0\n",
    "while OK:  #REPEAT UNTIL IT DO NOT IMPROVE, AT LEAST A LITLE, THE GINI IN 2 GENERATIONS\n",
    "    a=a+1\n",
    "    print ('loop ', a)\n",
    "    OK=0\n",
    "\n",
    "    ####\n",
    "    # GENERATING OFFSPRING - START\n",
    "    ####\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1) #CROSS-X PROBABILITY = 50%, MUTATION PROBABILITY=10%\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population =toolbox.select(offspring, k=len(population))\n",
    "    ####\n",
    "    # GENERATING OFFSPRING - END\n",
    "    ####\n",
    "\n",
    "    sum_current_gini_2=sum_current_gini_1\n",
    "    sum_current_gini_1=sum_current_gini\n",
    "    sum_current_gini=0.0\n",
    "\n",
    "    #####\n",
    "    #ASSESSING GINI ON THE OFFSPRING - START\n",
    "    #####\n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if population[j] not in dic_gini.values(): \n",
    "            var_model = [] \n",
    "            for i in range(np.shape(population)[0]): \n",
    "                if (population[j])[i]==1:\n",
    "                    var_model.append(list(list_inputs)[i])\n",
    "            \n",
    "            X_train= variables.copy()\n",
    "            Y_train= target.copy()\n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "            #####    \n",
    "    \n",
    "            \n",
    "            lr = RandomForestClassifier()\n",
    "            model = lr.fit(X_train, Y_train)\n",
    "            Y_predict = result.predict_proba(X_train)[:,1]\n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "            #####            \n",
    "                       \n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - START: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "            #####                       \n",
    "            fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict)\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            gini_power= 2*roc_auc_score(Y_train, Y_predict)-1   \n",
    "            #gini_power = abs(2*auc-1)\n",
    "            ######\n",
    "            # CHANGE_HERE - END: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "            #####                       \n",
    "           \n",
    "            gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "            dic_gini[gini]=population[j]  \n",
    "    #####\n",
    "    #ASSESSING GINI ON THE OFFSPRING - END\n",
    "    #####\n",
    "\n",
    "    #####\n",
    "    #SELECTING THE BEST FITTED AMONG ALL EVER CREATED POPULATION AND CURRENT OFFSPRING - START\n",
    "    #####           \n",
    "    list_gini=sorted(dic_gini.keys(),reverse=True)\n",
    "    population=[]\n",
    "    for i in list_gini[:NPOPSIZE]:\n",
    "        population.append(dic_gini[i])\n",
    "        gini=float(i.split(';')[0])\n",
    "        sum_current_gini+=gini\n",
    "    #####\n",
    "    #SELECTING THE BEST FITTED AMONG ALL EVER CREATED POPULATION AND CURRENT OFFSPRING - END\n",
    "    #####           \n",
    "      \n",
    "    #HAS IT IMPROVED AT LEAST A LITLE THE GINI IN THE LAST 2 GENERATIONS\n",
    "    print ('sum_current_gini=', sum_current_gini, 'sum_current_gini_1=', sum_current_gini_1, 'sum_current_gini_2=', sum_current_gini_2)\n",
    "    if(sum_current_gini>sum_current_gini_1+0.0001 or sum_current_gini>sum_current_gini_2+0.0001):\n",
    "        OK=1\n",
    "#####\n",
    "#GENETIC ALGORITHM MAIN LOOP - END\n",
    "#####\n",
    "\n",
    "\n",
    "gini_max=list_gini[0]        \n",
    "gini=float(gini_max.split(';')[0])\n",
    "features=gini_max.split(';')[1]\n",
    "\n",
    "\n",
    "####\n",
    "# PRINTING OUT THE LIST OF FEATURES\n",
    "#####\n",
    "adn_variables = []\n",
    "f=0\n",
    "for i in range(len(features)):\n",
    "    if features[i]=='1':\n",
    "        f+=1\n",
    "        adn_variables.append(list_inputs[i])\n",
    "        print ('feature ', f, ':', list(list_inputs)[i])\n",
    "print ('gini: ', gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametrización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def Grid_Search_CV_RFR(X_train, y_train):\n",
    "\n",
    "\n",
    "    estimator = RandomForestClassifier()\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [10, 20,30, 40, 50],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2,4,8,16],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"criterion\": ['entropy']\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    return grid.best_score_ , grid.best_params_\n",
    "\n",
    "def RFC(X_train, X_test, y_train, y_test, best_params):\n",
    "\n",
    "    estimator = RandomForestClassifier(n_jobs=-1).set_params(**best_params)\n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    print (\"GINI:\",GS(y_test,y_predict))\n",
    "    return y_test,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dejo comentado este codigo porque lleva mucho tiempo de ejecucion\n",
    "\n",
    "for i in range(5):\n",
    "    print (\"Loop: \" , i)\n",
    "    print (\"--------------\")\n",
    "    # Es importante dividir el set de datos en partes iguales 50% train y 50% test solo para evaluar\n",
    "    \n",
    "    best_score, best_params = Grid_Search_CV_RFR(x_train, y_train)\n",
    "    y_test , y_predict = RFC(x_train, x_test, y_train, y_test, best_params)\n",
    "    print (\"Best Score:\" ,best_score)\n",
    "    print (\"Best params:\",best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del loop anterior, para no volver a ejecutar que consume mucho tiempo:\n",
    "    \n",
    "\n",
    "Loop:  0\n",
    "--------------\n",
    "GINI: 0.46765917106241606\n",
    "Best Score: 0.7373068432671082\n",
    "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 4, 'n_estimators': 40}\n",
    "Loop:  1\n",
    "--------------\n",
    "GINI: 0.48113777793239576\n",
    "Best Score: 0.7350993377483444\n",
    "Best params: {'bootstrap': True, 'max_features': 'auto', 'min_samples_split': 2, 'n_estimators': 50}\n",
    "Loop:  2\n",
    "--------------\n",
    "GINI: 0.4689507168731546\n",
    "Best Score: 0.739514348785872\n",
    "Best params: {'bootstrap': False, 'max_features': 'sqrt', 'min_samples_split': 4, 'n_estimators': 40}\n",
    "Loop:  3\n",
    "--------------\n",
    "GINI: 0.494531586340055\n",
    "Best Score: 0.7380426784400295\n",
    "Best params: {'bootstrap': False, 'max_features': 'sqrt', 'min_samples_split': 16, 'n_estimators': 30}\n",
    "Loop:  4\n",
    "--------------\n",
    "GINI: 0.4992389881586554\n",
    "Best Score: 0.7373068432671082\n",
    "Best params: {'bootstrap': False, 'max_features': 'auto', 'min_samples_split': 16, 'n_estimators': 30}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte III: Evaluación del modelo\n",
    "\n",
    "## Enfoque sencillo de entrenamiento y test.\n",
    "\n",
    "### Modelos a evaluar:\n",
    "\n",
    "###### Random Forest Classifier\n",
    "###### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "variables = partidos[rf_variables].copy()\n",
    "\n",
    "variables, target = variables.iloc[:, :-1], variables.iloc[:, -1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = {'RFC': RandomForestClassifier(),\n",
    "              'LR': LogisticRegression()}\n",
    "\n",
    "print(\"ENTRENAMIENTO: Accuracy RFC, Accuracy LR:\")\n",
    "for mod in['RFC', 'LR']:\n",
    "    result = model_list[mod].fit(x_train, y_train)\n",
    "    y_pred = result.predict_proba(x_train)[:,1]\n",
    "    yo_pred = result.predict_proba(x_test)[:,1] #este es para tener la prob. cuando usamos skit.learn\n",
    "    print(result.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente a la elección del mejor modelo, creamos un dataframe de comparación entre y_test y y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe de resultados\n",
    "data_result = x_test.copy()\n",
    "data_result['target'] = y_test['target']\n",
    "data_result['pred'] = yo_pred\n",
    "\n",
    "\n",
    "# Columna de predicciones\n",
    "data_result['target_pred'] = 0\n",
    "wins = data_result['pred'] >= 0.50\n",
    "column = 'target_pred'\n",
    "data_result.loc[wins, column] =  1\n",
    "\n",
    "#exitos = len(data_result[data_result['target'] == data_result['target_pred']])\n",
    "#fallos = len(data_result[data_result['target'] != data_result['target_pred']])\n",
    "#exito / (exito + fallos)\n",
    "data_result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Validación Cruzada\n",
    "\n",
    "Una desventaja de usar un conjunto de datos de test y otro de entrenamiento para la validación del modelo es que hemos perdido una parte importante de nuestros datos (entre el 20% y 30%) en la capacitación del modelo. Esto no es óptimo desde muchos puntos de vista, principalmente por **uso inadecuado de los recursos escasos (datos)**.\n",
    "\n",
    "¿Pocos datos? Si, para el caso de la NBA es que existen muchos años de datos, sin embargo, a priori se tienen hipótesis de cambios estructurales (forma del juego) que de alguna forma el modelo no podrá diferenciar y es posible que no tenga la capacidad de generalizar sobre datos no observados.\n",
    "\n",
    "En este contexto, lo mejor que podemos hacer es aprovechar al máximo los datos que tenemos. Para resolver este problema utilizaremos la validación cruzada.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**NOTA:** como parte del proyecto será importante plantear la hipótesis sobre el posible cambio estructural de los datos dado el cambio de juego (posiblemente un juego mas rapido, mas tiros de triples, mas puntos anotados, etc).\n",
    "\n",
    "Esta hipótesis sobre la generealizacion del modelo se puede abordar realizando un modelo con menos datos y otro con mas datos para posteriormente comparar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la función `cross_val_score`. Esta función divide el conjunto de datos en diferentes muestras y entrena con todas menos una, dejando la restante para validación. Este proceso se repite según el número en el que se divida las muestras para la validación.\n",
    "\n",
    "Otro problema de utilizar esta técnica es saber **cuál es el número óptimo** de divisiones de la muestra.\n",
    "\n",
    "Estas divisiones de muestra se llaman **folds** determinada por la letra **\"k\"**. Dentro de los parámetros de `cross_val_score` se usan dos métodos para la division de la muestra,  KFold y StratifiedKFold.\n",
    "\n",
    "Por ahora utilizaremos como base el enfoque más común en problemas de Machine Learning determinando k= 10 sabiendo que este parámetro bien puede ir desde **k= 2** hasta **k = n** donde \"n\" el tamaño de los datos.\n",
    "\n",
    "\n",
    "Mediante `cv` se indica el numero de partes en las que se dividen los datos. La función devuelve un vector con el score de cada uno de los modelos construidos.\n",
    "\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteracion entre modelos para comparación\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for mod in['RFC', 'LR']:\n",
    "    scores = cross_val_score(model_list[mod], x_train, y_train, cv = 5) #cv variacion cruzada 10 modelos\n",
    "    print(\"Vector de Accuracy para cada CV del modelo de ENTRENAMIENTO\")\n",
    "    print(\"\")\n",
    "    print(scores)\n",
    "    print(\"\")\n",
    "    print(\"Vector de Accuracy de media y desviación típica de ENTRENAMIENTO\")\n",
    "    print(\"\")   \n",
    "    print(np.mean(scores), np.std(scores))\n",
    "    print(\"\")\n",
    "    print(\"-------------------------------------------------------------------\") \n",
    "# Primer output RF\n",
    "# Segundo output LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: \n",
    "\n",
    "Si existe un mal desempeño del modelo en el resultado anterior, se debe evaluar métodos que limiten la caida del accuracy o cualquier otra métrica de evaluación en los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteracion entre modelos para comparación\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "for mod in['RFC', 'LR']:\n",
    "    scores = cross_val_score(model_list[mod], x_test, y_test, cv = 5) # output del mejor modelo\n",
    "    print(\"Accuracy del modelo en TEST usando Validación Cruzada seleccinando el mejor de k = 5\")\n",
    "    print(scores)\n",
    "    print(\"-------------------------------------------------------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir un proceso para almacenar ambos resultados (entrenamiento y test) en dataframes (mejor visualizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nota mental: buscar donde leí sobre el re-entrenamiento tantas veces como \"k\" folds elegimos.\n",
    "# segun me acuerdo, esto permite que el modelo aprenda muy bien el patron de los datos.\n",
    "# no es solo usar CV como método de validacion y ya, sino, usar esos procesos iterativos para que el modelo\n",
    "# aprenda en cada iteracion los datos. Por eso digo un re-entrenamiento.\n",
    "# De no hacer esto, tal como lo estoy haciendo ahora, solo estoy usando CV para validar, luego descarto ese proceso \n",
    "# y voy enseguida a entrenar el modelo con el 70% del train y luego evaluo en 30% y ya..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
