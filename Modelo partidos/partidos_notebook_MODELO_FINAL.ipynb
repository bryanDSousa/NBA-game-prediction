{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glosario de términos\n",
    "\n",
    "MP -- Minutes Played\n",
    "\n",
    "FG -- Field Goals\n",
    "\n",
    "FGA -- Field Goal Attempts\n",
    "\n",
    "FG% -- Field Goal Percentage\n",
    "\n",
    "3P -- 3-Point Field Goals\n",
    "\n",
    "3PA -- 3-Point Field Goal Attempts\n",
    "\n",
    "3P% -- 3-Point Field Goal Percentage\n",
    "\n",
    "FT -- Free Throws\n",
    "\n",
    "FTA -- Free Throw Attempts\n",
    "\n",
    "FT% -- Free Throw Percentage\n",
    "\n",
    "ORB -- Offensive Rebounds\n",
    "\n",
    "DRB -- Defensive Rebounds\n",
    "\n",
    "TRB -- Total Rebounds\n",
    "\n",
    "AST -- Assists\n",
    "\n",
    "STL -- Steals\n",
    "\n",
    "BLK -- Blocks\n",
    "\n",
    "TOV -- Turnovers\n",
    "\n",
    "PF -- Personal Fouls\n",
    "\n",
    "PTS -- Points\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "OFF RATE: **offensive rating**  is a statistic used to measure either a teams offensive performance or an individual player's efficiency at producing points.\n",
    "\n",
    "**Defensive Player Rating** = (Players Points.Total FG%) + **Opponents Differential**= 1/5 of possessions - Times Fouled+ FTM* FT% * OAPOW( Official Adjusted Players Offensive Withstand). \n",
    "\n",
    "***********************************************************************************************\n",
    "\n",
    "DEFF RATE: **Defensive rating** is a statistic used to measure an individual player's efficiency at preventing the other team from scoring points\n",
    "\n",
    "**Defensive Player Rating** = (Players Steals.Blocks) + **Opponents Differential**= 1/5 of possessions - Times blown by + Deflections * OAPDW( Official Adjusted Players Defensive Withstand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from scipy import stats\n",
    "warnings.filterwarnings(action='ignore')\n",
    "%matplotlib inline\n",
    "%pylab\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte I: Exploración, limpieza, selección de variables, entre otros.\n",
    "## Analisis exploratorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Season</th>\n",
       "      <th>local_team</th>\n",
       "      <th>visitor_team</th>\n",
       "      <th>Points</th>\n",
       "      <th>Opponent_Points</th>\n",
       "      <th>Result</th>\n",
       "      <th>LOCAL_Racha</th>\n",
       "      <th>...</th>\n",
       "      <th>local_played_visitor</th>\n",
       "      <th>visitor_played_visitor</th>\n",
       "      <th>visitor_played_local</th>\n",
       "      <th>place_local_played</th>\n",
       "      <th>place_visitor_played</th>\n",
       "      <th>avg_distance_place_visitor_played</th>\n",
       "      <th>avg_distance_place_local_played</th>\n",
       "      <th>distance_between_stadiums</th>\n",
       "      <th>distance_local_traveled</th>\n",
       "      <th>distance_visitor_traveled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Oct 30</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Utah Jazz</td>\n",
       "      <td>71</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Boston Celtics</td>\n",
       "      <td>Detroit Pistons</td>\n",
       "      <td>313.5</td>\n",
       "      <td>822.000000</td>\n",
       "      <td>2964.0</td>\n",
       "      <td>1644.0</td>\n",
       "      <td>627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Nov 2</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>100</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>488.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1464.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1464.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Nov 7</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>97</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>-5</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cleveland Cavaliers</td>\n",
       "      <td>Orlando Magic</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>1464.000000</td>\n",
       "      <td>1806.0</td>\n",
       "      <td>1464.0</td>\n",
       "      <td>1806.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Nov 9</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>88</td>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>-6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1002.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Nov 11</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>103</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>-7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>301.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Nov 16</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Dallas Mavericks</td>\n",
       "      <td>86</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "      <td>-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>Houston Rockets</td>\n",
       "      <td>285.0</td>\n",
       "      <td>3841.000000</td>\n",
       "      <td>314.0</td>\n",
       "      <td>7682.0</td>\n",
       "      <td>570.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Nov 18</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Indiana Pacers</td>\n",
       "      <td>85</td>\n",
       "      <td>112</td>\n",
       "      <td>1</td>\n",
       "      <td>-11</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Chicago Bulls</td>\n",
       "      <td>501.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>990.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Dec 1</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>103</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>-18</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Memphis Grizzlies</td>\n",
       "      <td>Los Angeles Lakers</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>135.500000</td>\n",
       "      <td>4006.0</td>\n",
       "      <td>271.0</td>\n",
       "      <td>4006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Dec 5</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>Denver Nuggets</td>\n",
       "      <td>105</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>New York Knicks</td>\n",
       "      <td>Toronto Raptors</td>\n",
       "      <td>150.5</td>\n",
       "      <td>135.666667</td>\n",
       "      <td>1349.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Dec 7</td>\n",
       "      <td>2015</td>\n",
       "      <td>2016</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>68</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Philadelphia 76ers</td>\n",
       "      <td>San Antonio Spurs</td>\n",
       "      <td>3841.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7682.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7682.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Date  Year  Season          local_team         visitor_team  \\\n",
       "0           0  Oct 30  2015    2016  Philadelphia 76ers            Utah Jazz   \n",
       "1           1   Nov 2  2015    2016  Philadelphia 76ers  Cleveland Cavaliers   \n",
       "2           2   Nov 7  2015    2016  Philadelphia 76ers        Orlando Magic   \n",
       "3           3   Nov 9  2015    2016  Philadelphia 76ers        Chicago Bulls   \n",
       "4           4  Nov 11  2015    2016  Philadelphia 76ers      Toronto Raptors   \n",
       "5           5  Nov 16  2015    2016  Philadelphia 76ers     Dallas Mavericks   \n",
       "6           6  Nov 18  2015    2016  Philadelphia 76ers       Indiana Pacers   \n",
       "7           7   Dec 1  2015    2016  Philadelphia 76ers   Los Angeles Lakers   \n",
       "8           8   Dec 5  2015    2016  Philadelphia 76ers       Denver Nuggets   \n",
       "9           9   Dec 7  2015    2016  Philadelphia 76ers    San Antonio Spurs   \n",
       "\n",
       "   Points  Opponent_Points  Result  LOCAL_Racha  ... local_played_visitor  \\\n",
       "0      71               99       1           -1  ...                    1   \n",
       "1     100              107       1           -2  ...                    0   \n",
       "2      97              105       1           -5  ...                    1   \n",
       "3      88              111       1           -6  ...                    0   \n",
       "4     103              119       1           -7  ...                    0   \n",
       "5      86               92       1          -10  ...                    1   \n",
       "6      85              112       1          -11  ...                    0   \n",
       "7     103               91       0          -18  ...                    1   \n",
       "8     105              108       1           -1  ...                    1   \n",
       "9      68              119       1           -2  ...                    0   \n",
       "\n",
       "   visitor_played_visitor  visitor_played_local   place_local_played  \\\n",
       "0                       1                     0       Boston Celtics   \n",
       "1                       0                     1   Philadelphia 76ers   \n",
       "2                       0                     1  Cleveland Cavaliers   \n",
       "3                       0                     1   Philadelphia 76ers   \n",
       "4                       0                     1   Philadelphia 76ers   \n",
       "5                       1                     0    San Antonio Spurs   \n",
       "6                       1                     0   Philadelphia 76ers   \n",
       "7                       0                     1    Memphis Grizzlies   \n",
       "8                       1                     0      New York Knicks   \n",
       "9                       0                     1   Philadelphia 76ers   \n",
       "\n",
       "   place_visitor_played  avg_distance_place_visitor_played  \\\n",
       "0       Detroit Pistons                              313.5   \n",
       "1   Cleveland Cavaliers                              488.0   \n",
       "2         Orlando Magic                             1806.0   \n",
       "3         Chicago Bulls                              501.0   \n",
       "4       Toronto Raptors                              301.0   \n",
       "5       Houston Rockets                              285.0   \n",
       "6         Chicago Bulls                              501.0   \n",
       "7    Los Angeles Lakers                             2003.0   \n",
       "8       Toronto Raptors                              150.5   \n",
       "9     San Antonio Spurs                             3841.0   \n",
       "\n",
       "   avg_distance_place_local_played  distance_between_stadiums  \\\n",
       "0                       822.000000                     2964.0   \n",
       "1                         0.000000                     1464.0   \n",
       "2                      1464.000000                     1806.0   \n",
       "3                         0.000000                     1002.0   \n",
       "4                         0.000000                      301.0   \n",
       "5                      3841.000000                      314.0   \n",
       "6                         0.000000                      990.0   \n",
       "7                       135.500000                     4006.0   \n",
       "8                       135.666667                     1349.0   \n",
       "9                         0.000000                     7682.0   \n",
       "\n",
       "   distance_local_traveled  distance_visitor_traveled  \n",
       "0                   1644.0                      627.0  \n",
       "1                      0.0                     1464.0  \n",
       "2                   1464.0                     1806.0  \n",
       "3                      0.0                     1002.0  \n",
       "4                      0.0                      301.0  \n",
       "5                   7682.0                      570.0  \n",
       "6                      0.0                     1002.0  \n",
       "7                    271.0                     4006.0  \n",
       "8                    407.0                      301.0  \n",
       "9                      0.0                     7682.0  \n",
       "\n",
       "[10 rows x 116 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('../Nuevas_variables/v5/input_extendidov5.csv')\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6220"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5891"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.dropna()\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0\n",
      "Date\n",
      "Year\n",
      "Season\n",
      "local_team\n",
      "visitor_team\n",
      "Points\n",
      "Opponent_Points\n",
      "Result\n",
      "LOCAL_Racha\n",
      "ID Partido\n",
      "LOCAL_Ultimos10Victorias\n",
      "LOCAL_Ultimos10Derrotas\n",
      "VISITANTE_Ultimos10Victorias\n",
      "VISITANTE_Ultimos10Derrotas\n",
      "VISITANTE_Racha\n",
      "local_Conf_position\n",
      "local_Win\n",
      "local_Lose\n",
      "local_Percentagewl\n",
      "local_Dif_leader\n",
      "local_Home_win\n",
      "local_Home_lose\n",
      "local_Away_win\n",
      "local_Away_lose\n",
      "local_Div_win\n",
      "local_Div_lose\n",
      "local_Cnf_win\n",
      "local_Cnf_lose\n",
      "local_Icf_win\n",
      "local_Icf_lose\n",
      "visitor_Conf_position\n",
      "visitor_Win\n",
      "visitor_Lose\n",
      "visitor_Percentagewl\n",
      "visitor_Dif_leader\n",
      "visitor_Home_win\n",
      "visitor_Home_lose\n",
      "visitor_Away_win\n",
      "visitor_Away_lose\n",
      "visitor_Div_win\n",
      "visitor_Div_lose\n",
      "visitor_Cnf_win\n",
      "visitor_Cnf_lose\n",
      "visitor_Icf_win\n",
      "visitor_Icf_lose\n",
      "local_fg\n",
      "local_fga\n",
      "local_fg3\n",
      "local_fg3a\n",
      "local_ft\n",
      "local_fta\n",
      "local_orb\n",
      "local_drb\n",
      "local_trb\n",
      "local_ast\n",
      "local_stl\n",
      "local_blk\n",
      "local_tov\n",
      "local_pf\n",
      "local_pts\n",
      "local_ft_pct\n",
      "local_fg_pct\n",
      "local_fg3_pct\n",
      "visitor_fg\n",
      "visitor_fga\n",
      "visitor_fg3\n",
      "visitor_fg3a\n",
      "visitor_ft\n",
      "visitor_fta\n",
      "visitor_orb\n",
      "visitor_drb\n",
      "visitor_trb\n",
      "visitor_ast\n",
      "visitor_stl\n",
      "visitor_blk\n",
      "visitor_tov\n",
      "visitor_pf\n",
      "visitor_pts\n",
      "visitor_ft_pct\n",
      "visitor_fg_pct\n",
      "visitor_fg3_pct\n",
      "Sueldo local\n",
      "Sueldo visitante\n",
      "Local_Division\n",
      "Local_Conferencia\n",
      "Visitor_Division\n",
      "Visitor_Conferencia\n",
      "LOCAL_AWS_MEDIO_AGRUPADO\n",
      "VISITOR_AWS_MEDIO_AGRUPADO\n",
      "number_date\n",
      "local_dif_between_previous_game\n",
      "visitor_dif_between_previous_game\n",
      "local_played_previous_date\n",
      "visitor_played_previous_date\n",
      "local_played_two_days_ago\n",
      "visitor_played_two_days_ago\n",
      "local_played_three_days_ago\n",
      "visitor_played_three_days_ago\n",
      "local_played_prorrogue_previous_date\n",
      "visitor_played_prorrogue_previous_date\n",
      "local_played_prorrogue_two_days_ago\n",
      "visitor_played_prorrogue_two_days_ago\n",
      "local_played_prorrogue_three_days_ago\n",
      "visitor_played_prorrogue_three_days_ago\n",
      "local_played_local\n",
      "local_played_visitor\n",
      "visitor_played_visitor\n",
      "visitor_played_local\n",
      "place_local_played\n",
      "place_visitor_played\n",
      "avg_distance_place_visitor_played\n",
      "avg_distance_place_local_played\n",
      "distance_between_stadiums\n",
      "distance_local_traveled\n",
      "distance_visitor_traveled\n"
     ]
    }
   ],
   "source": [
    "for column in data.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculo de variables compuestas\n",
    "\n",
    "Indicar en el estudio la seleccion de variables por logica del modelo, y comentar aquellas observadas en estudios previos que no tienen ningun sentido incluir\n",
    "\n",
    "Por ejemplo la ilogica inclusion de la diferencia_visitante_point observado en estudios previos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLE MAL CALCULADA, ESPERA DE ARREGLO POR PARTE DE VICTOR\n",
    "data[\"Result1\"] = 1\n",
    "\n",
    "data[\"Result1\"] = np.where(data[\"Opponent_Points\"] > data[\"Points\"], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMENTAR A VICTOR ARREGLAR EL NOMBRE DE LAS VARIABLES\n",
    "\n",
    "\n",
    "data.rename(columns={'Result1': 'target'}, inplace=True)\n",
    "\n",
    "\n",
    "data['local_visitor_dif_pts'] = data['local_pts'] - data['visitor_pts']\n",
    "\n",
    "data['LOCAL_porcentaje_victoria_Ultimos10'] = (data['LOCAL_Ultimos10Victorias']) / (data['LOCAL_Ultimos10Victorias'] + data['LOCAL_Ultimos10Derrotas'])\n",
    "data['VISITANTE_porcentaje_victoria_Ultimos10'] = (data['VISITANTE_Ultimos10Victorias']) / (data['VISITANTE_Ultimos10Victorias'] + data['VISITANTE_Ultimos10Derrotas'])\n",
    "\n",
    "# COMENTAR A VICTOR SI ES POSIBLE SACAR ESTAS VARIABLES, PREVIAMENTE SON MAS IMPORTANTES QUE \n",
    "# LOCAL_porcentajeVictorias O local_Percentagewl\n",
    "\n",
    "data['LOCAL_porcentaje_victoria_LOCAL'] = (data['local_Home_win']) / (data['local_Home_win'] + data['local_Home_lose'])\n",
    "data['VISITANTE_porcentaje_victoria_VISITANTE'] = (data['visitor_Home_win']) / (data['visitor_Home_win'] + data['visitor_Home_lose'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5809"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aqui pasa algo raro con los denominadores de las variables que hace INDETERMINADO los calculos y lleva luego a NA\n",
    "\n",
    "data = data.dropna()\n",
    "len(data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion de variables categoricas\n",
    "\n",
    "Estrictamente necesario transformar las variables categoricas para modelar con las librerias de Python que estamos utilizando.\n",
    "\n",
    "Distintos enfoques para trabajar variables categoricas: https://pbpython.com/categorical-encoding.html\n",
    "\n",
    "Trabajar con variables categoricas en Machine Learning: https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/\n",
    "\n",
    "\n",
    "## Investigar mas a fondo sobre:\n",
    "## \"Maldición de la dimensionalidad\"\n",
    "\n",
    "¿Un posible problema por la escasez de datos y la alta dimensionalidad del modelo? Cerca de 100 variables previo a selección (pendiente de profundizar) \n",
    "\n",
    "https://en.wikipedia.org/wiki/Curse_of_dimensionality#Machine_learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['Local_Conferencia', 'Local_Division'],\n",
    "               prefix=['Conf_local', 'Div_local'])\n",
    "\n",
    "data = pd.get_dummies(data, columns=['Visitor_Conferencia', 'Visitor_Division'],\n",
    "               prefix=['Conf_visitor', 'Div_visitor'])\n",
    "\n",
    "data.rename(columns={'Div_local_Atlantic Division':'Div_local_Atlantic',\n",
    "                    'Div_local_Central Division': 'Div_local_Central',\n",
    "                    'Div_local_Northwest Division': 'Div_local_Northwest',\n",
    "                    'Div_local_Pacific Division': 'Div_local_Pacific',\n",
    "                    'Div_local_Southeast Division': 'Div_local_Southeast',\n",
    "                    'Div_local_Southwest Division': 'Div_local_Southwest'},inplace=True)\n",
    "\n",
    "data.rename(columns={'Div_visitor_Atlantic Division':'Div_visitor_Atlantic',\n",
    "                    'Div_visitor_Central Division': 'Div_visitor_Central',\n",
    "                    'Div_visitor_Northwest Division': 'Div_visitor_Northwest',\n",
    "                    'Div_visitor_Pacific Division': 'Div_visitor_Pacific',\n",
    "                    'Div_visitor_Southeast Division': 'Div_visitor_Southeast',\n",
    "                    'Div_visitor_Southwest Division': 'Div_visitor_Southwest'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['local_team', 'LOCAL_Racha', 'LOCAL_Ultimos10Victorias', 'LOCAL_Ultimos10Derrotas', 'local_Conf_position', 'local_Win', 'local_Lose', 'local_Percentagewl', 'local_Dif_leader', 'local_Home_win', 'local_Home_lose', 'local_Away_win', 'local_Away_lose', 'local_Div_win', 'local_Div_lose', 'local_Cnf_win', 'local_Cnf_lose', 'local_Icf_win', 'local_Icf_lose', 'local_fg', 'local_fga', 'local_fg3', 'local_fg3a', 'local_ft', 'local_fta', 'local_orb', 'local_drb', 'local_trb', 'local_ast', 'local_stl', 'local_blk', 'local_tov', 'local_pf', 'local_pts', 'local_ft_pct', 'local_fg_pct', 'local_fg3_pct', 'Sueldo local', 'LOCAL_AWS_MEDIO_AGRUPADO', 'local_dif_between_previous_game', 'local_played_previous_date', 'local_played_two_days_ago', 'local_played_three_days_ago', 'local_played_prorrogue_previous_date', 'local_played_prorrogue_two_days_ago', 'local_played_prorrogue_three_days_ago', 'local_played_local', 'local_played_visitor', 'visitor_played_local', 'place_local_played', 'avg_distance_place_local_played', 'distance_local_traveled', 'local_visitor_dif_pts', 'LOCAL_porcentaje_victoria_Ultimos10', 'LOCAL_porcentaje_victoria_LOCAL', 'Conf_local_Este', 'Conf_local_Oeste', 'Div_local_Atlantic', 'Div_local_Central', 'Div_local_Northwest', 'Div_local_Pacific', 'Div_local_Southeast', 'Div_local_Southwest']\n"
     ]
    }
   ],
   "source": [
    "# Seleccion de variables para análisis de correlación\n",
    "\n",
    "local = []\n",
    "def filter_local_name(name):\n",
    "    if 'Local' in name:\n",
    "      return name\n",
    "    if 'LOCAL' in name:\n",
    "        return name\n",
    "    if 'local' in name:\n",
    "        return name\n",
    "    \n",
    "for variable in data.columns:\n",
    "    if (filter_local_name(variable) != None):\n",
    "        local.append(variable)\n",
    "print(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NO LO VEO. Como mucho si se quiere se puede calcular si en los ultimos 10 partidos se enfrentaron estos equipos\n",
    "#data['place_local_played'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccion de Variables por Matriz de Correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copiamos y pegamos el output del código anterior, incluyendo ahora \"target\"\n",
    "\n",
    "game_var_local = ['target', 'local_pts', 'local_visitor_dif_pts', 'local_fg', 'local_fga', 'local_fg3', \n",
    "                  'local_fg3a', 'local_ft', 'local_fta', 'local_orb', 'local_drb', 'local_trb', 'local_ast',\n",
    "                  'local_stl', 'local_blk', 'local_tov','local_pf','local_ft_pct', 'local_fg_pct', 'local_fg3_pct']\n",
    "\n",
    "game_var_visitante = ['target', 'visitor_pts', 'local_visitor_dif_pts', 'visitor_fg', 'visitor_fga', \n",
    "                    'visitor_fg3', 'visitor_fg3a', 'visitor_ft', 'visitor_fta', 'visitor_orb', 'visitor_drb',\n",
    "                    'visitor_trb', 'visitor_ast','visitor_stl', 'visitor_blk', 'visitor_tov','visitor_pf',\n",
    "                    'visitor_ft_pct', 'visitor_fg_pct', 'visitor_fg3_pct']\n",
    "\n",
    "\n",
    "prev_var_local = ['target', 'LOCAL_Racha', 'LOCAL_Ultimos10Victorias', 'LOCAL_Ultimos10Derrotas',\n",
    "                  'LOCAL_porcentaje_victoria_Ultimos10', 'local_Home_win', 'local_Home_lose', 'local_Percentagewl',\n",
    "                  'LOCAL_porcentaje_victoria_LOCAL','local_Conf_position','local_Dif_leader','Sueldo local',\n",
    "                  \n",
    "                  'local_dif_between_previous_game', 'local_played_local', \n",
    "                  'local_played_visitor', 'avg_distance_place_local_played',\n",
    "                  'distance_between_stadiums', 'distance_local_traveled', 'LOCAL_AWS_MEDIO_AGRUPADO']\n",
    "\n",
    "\n",
    "prev_var_visitante = ['target', 'VISITANTE_Racha', 'VISITANTE_Ultimos10Victorias', 'VISITANTE_Ultimos10Derrotas',\n",
    "                      'VISITANTE_porcentaje_victoria_Ultimos10', 'visitor_Home_win', 'visitor_Home_lose',\n",
    "                      'visitor_Percentagewl','VISITANTE_porcentaje_victoria_VISITANTE','visitor_Conf_position',\n",
    "                      'visitor_Dif_leader','Sueldo visitante',\n",
    "                      \n",
    "                      'visitor_dif_between_previous_game', 'visitor_played_local', \n",
    "                      'visitor_played_visitor', 'avg_distance_place_visitor_played',\n",
    "                      'distance_between_stadiums', 'distance_visitor_traveled', 'VISITOR_AWS_MEDIO_AGRUPADO']\n",
    "\n",
    "\n",
    "dummy = ['local_played_previous_date','local_played_two_days_ago','local_played_three_days_ago',\n",
    "         'local_played_prorrogue_previous_date','local_played_prorrogue_two_days_ago',\n",
    "         'local_played_prorrogue_three_days_ago','Conf_local_Este','Div_local_Atlantic','Div_local_Central',\n",
    "         'Div_local_Northwest', 'Div_local_Pacific','Div_local_Southeast', 'Div_local_Southwest',\n",
    "         \n",
    "         'Conf_visitor_Este','Div_visitor_Atlantic','Div_visitor_Central','Div_visitor_Northwest',\n",
    "         'Div_visitor_Pacific','Div_visitor_Southeast', 'Div_visitor_Southwest','visitor_played_previous_date',\n",
    "         'visitor_played_two_days_ago','visitor_played_three_days_ago','visitor_played_prorrogue_previous_date',\n",
    "         'visitor_played_prorrogue_two_days_ago', 'visitor_played_prorrogue_three_days_ago']\n",
    "\n",
    "\n",
    "\n",
    "prev_data_local = data[prev_var_local]\n",
    "prev_data_visitante = data[prev_var_visitante]\n",
    "\n",
    "game_data_local = data[game_var_local]\n",
    "game_data_visitante = data[game_var_visitante]\n",
    "\n",
    "all_variables = game_var_local + game_var_visitante + prev_var_local + prev_var_visitante +dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variables continuas:  99\n",
      "Total variables dummy:  26\n"
     ]
    }
   ],
   "source": [
    "all_variables = list(dict.fromkeys(all_variables)) #drop duplicates\n",
    "del all_variables[0] #borramos target de la primera posicion\n",
    "\n",
    "all_variables.append('target') #insertamos target en la ultima posicion\n",
    "\n",
    "print('Total variables continuas:  '+ str(len(all_variables)))\n",
    "print('Total variables dummy:  '+ str(len(dummy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['local_pts',\n",
       " 'local_visitor_dif_pts',\n",
       " 'local_fg',\n",
       " 'local_fga',\n",
       " 'local_fg3',\n",
       " 'local_fg3a',\n",
       " 'local_ft',\n",
       " 'local_fta',\n",
       " 'local_orb',\n",
       " 'local_drb',\n",
       " 'local_trb',\n",
       " 'local_ast',\n",
       " 'local_stl',\n",
       " 'local_blk',\n",
       " 'local_tov',\n",
       " 'local_pf',\n",
       " 'local_ft_pct',\n",
       " 'local_fg_pct',\n",
       " 'local_fg3_pct',\n",
       " 'visitor_pts',\n",
       " 'visitor_fg',\n",
       " 'visitor_fga',\n",
       " 'visitor_fg3',\n",
       " 'visitor_fg3a',\n",
       " 'visitor_ft',\n",
       " 'visitor_fta',\n",
       " 'visitor_orb',\n",
       " 'visitor_drb',\n",
       " 'visitor_trb',\n",
       " 'visitor_ast',\n",
       " 'visitor_stl',\n",
       " 'visitor_blk',\n",
       " 'visitor_tov',\n",
       " 'visitor_pf',\n",
       " 'visitor_ft_pct',\n",
       " 'visitor_fg_pct',\n",
       " 'visitor_fg3_pct',\n",
       " 'LOCAL_Racha',\n",
       " 'LOCAL_Ultimos10Victorias',\n",
       " 'LOCAL_Ultimos10Derrotas',\n",
       " 'LOCAL_porcentaje_victoria_Ultimos10',\n",
       " 'local_Home_win',\n",
       " 'local_Home_lose',\n",
       " 'local_Percentagewl',\n",
       " 'LOCAL_porcentaje_victoria_LOCAL',\n",
       " 'local_Conf_position',\n",
       " 'local_Dif_leader',\n",
       " 'Sueldo local',\n",
       " 'local_dif_between_previous_game',\n",
       " 'local_played_local',\n",
       " 'local_played_visitor',\n",
       " 'avg_distance_place_local_played',\n",
       " 'distance_between_stadiums',\n",
       " 'distance_local_traveled',\n",
       " 'LOCAL_AWS_MEDIO_AGRUPADO',\n",
       " 'VISITANTE_Racha',\n",
       " 'VISITANTE_Ultimos10Victorias',\n",
       " 'VISITANTE_Ultimos10Derrotas',\n",
       " 'VISITANTE_porcentaje_victoria_Ultimos10',\n",
       " 'visitor_Home_win',\n",
       " 'visitor_Home_lose',\n",
       " 'visitor_Percentagewl',\n",
       " 'VISITANTE_porcentaje_victoria_VISITANTE',\n",
       " 'visitor_Conf_position',\n",
       " 'visitor_Dif_leader',\n",
       " 'Sueldo visitante',\n",
       " 'visitor_dif_between_previous_game',\n",
       " 'visitor_played_local',\n",
       " 'visitor_played_visitor',\n",
       " 'avg_distance_place_visitor_played',\n",
       " 'distance_visitor_traveled',\n",
       " 'VISITOR_AWS_MEDIO_AGRUPADO',\n",
       " 'local_played_previous_date',\n",
       " 'local_played_two_days_ago',\n",
       " 'local_played_three_days_ago',\n",
       " 'local_played_prorrogue_previous_date',\n",
       " 'local_played_prorrogue_two_days_ago',\n",
       " 'local_played_prorrogue_three_days_ago',\n",
       " 'Conf_local_Este',\n",
       " 'Div_local_Atlantic',\n",
       " 'Div_local_Central',\n",
       " 'Div_local_Northwest',\n",
       " 'Div_local_Pacific',\n",
       " 'Div_local_Southeast',\n",
       " 'Div_local_Southwest',\n",
       " 'Conf_visitor_Este',\n",
       " 'Div_visitor_Atlantic',\n",
       " 'Div_visitor_Central',\n",
       " 'Div_visitor_Northwest',\n",
       " 'Div_visitor_Pacific',\n",
       " 'Div_visitor_Southeast',\n",
       " 'Div_visitor_Southwest',\n",
       " 'visitor_played_previous_date',\n",
       " 'visitor_played_two_days_ago',\n",
       " 'visitor_played_three_days_ago',\n",
       " 'visitor_played_prorrogue_previous_date',\n",
       " 'visitor_played_prorrogue_two_days_ago',\n",
       " 'visitor_played_prorrogue_three_days_ago',\n",
       " 'target']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “Garbage in, garbage out”\n",
    "\n",
    "Hemos eliminado las siguientes variables previo al análisis, se sabe a priori que dado el estudio no enriquecen el modelo en ninguna manera (LOCAL Y VISITANTE):\n",
    "\n",
    "**'local_team', 'local_Win', 'local_Lose', 'local_Away_win', 'local_Away_lose', 'local_Div_win', 'local_Div_lose', 'local_Cnf_win', 'local_Cnf_lose', 'local_Icf_win', 'local_Icf_lose'**\n",
    "\n",
    "**'Conf_local_Oeste'** por defecto tiene colinealidad perfecta con 'Conf_local_Este'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.artist.Artist.remove(self)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_data_local.hist(figsize = (20, 20))\n",
    "matplotlib.axes.Axes.remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.artist.Artist.remove(self)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game_data_local.hist(figsize = (20, 20))\n",
    "matplotlib.axes.Axes.remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Matriz de Correlación Previo al Partido LOCAL')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heatmap PREV LOCAL\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns = sns.heatmap(prev_data_local.corr(), annot=True, fmt='.2f')\n",
    "sns.set_title('Matriz de Correlación Previo al Partido LOCAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Matriz de Correlación Previo al Partido VISITANTE')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heatmap PREV VISITANTE\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_2 = sns.heatmap(prev_data_visitante.corr(), annot=True, fmt='.2f')\n",
    "sns_2.set_title('Matriz de Correlación Previo al Partido VISITANTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Matriz de Correlación Durante el Partido LOCAL')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heatmap GAME LOCAL\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_3 = sns.heatmap(game_data_local.corr(), annot=True, fmt='.2f')\n",
    "sns_3.set_title('Matriz de Correlación Durante el Partido LOCAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Matriz de Correlación Durante el Partido VISITANTE')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heatmap GAME VISITANTE\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns_3 = sns.heatmap(game_data_visitante.corr(), annot=True, fmt='.2f')\n",
    "sns_3.set_title('Matriz de Correlación Durante el Partido VISITANTE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "partidos = data[all_variables]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables Dummy\n",
    "\n",
    "Usando el test de independencia de chi-cuadrado y Ganancia de la Información\n",
    "\n",
    "**¿Incluir la totalidad de datos en la selección de variables? o ¿solo seleccionar las variables con los datos de entrenamiento?**\n",
    "\n",
    "Puede parecer una pregunta tonta y obvia en la respuesta, pero no lo es.\n",
    "\n",
    "Muchos expertos en el tema debaten respecto a esto, y en mi caso, sobre el último modelo que trabajé para la detección de fraude bancario, el proceso de selección se realizó con la totalidad de los datos (por recomendaciones del profesor, creo que este tipo de modelos son \"especiales\").\n",
    "\n",
    "En este aspecto, y por evitar problemas de cualquier índole (por ejemplo el mayor argumento cuando se trabaja con modelos supervisados de clasificacion es tener resultados inflados en TEST), se realiza la selección solo con los datos de entrenamiento.\n",
    "\n",
    "https://stackoverflow.com/questions/56308116/should-feature-selection-be-done-before-train-test-split-or-after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "variables, target = partidos.iloc[:, :-1], partidos.iloc[:, -1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4066, 98)\n",
      "(1743, 98)\n",
      "(4066, 1)\n",
      "(1743, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "fs = SelectKBest(score_func=chi2, k='all')\n",
    "fs.fit(x_train[dummy], y_train)\n",
    "x_train_fs = fs.transform(x_train[dummy])\n",
    "x_test_fs = fs.transform(x_test[dummy])\n",
    "\n",
    "mask = fs.get_support()\n",
    "chi2_variables = x_train[dummy].columns[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: local_played_previous_date   0.004792194693426954\n",
      "1: local_played_two_days_ago   0.10248628422191709\n",
      "2: local_played_three_days_ago   0.09139203456491637\n",
      "3: local_played_prorrogue_previous_date   1.8318007587577025\n",
      "4: local_played_prorrogue_two_days_ago   6.384589388365216\n",
      "5: local_played_prorrogue_three_days_ago   1.8274951742062266\n",
      "6: Conf_local_Este   1.0416880877334551\n",
      "7: Div_local_Atlantic   0.6685286135548474\n",
      "8: Div_local_Central   1.113761663264697\n",
      "9: Div_local_Northwest   7.152926129295153\n",
      "10: Div_local_Pacific   4.169742433124352\n",
      "11: Div_local_Southeast   4.128075992114299\n",
      "12: Div_local_Southwest   1.02765747798342\n",
      "13: Conf_visitor_Este   2.684004972606048\n",
      "14: Div_visitor_Atlantic   0.0009053860219243146\n",
      "15: Div_visitor_Central   1.079508061385642\n",
      "16: Div_visitor_Northwest   1.4233913859265637\n",
      "17: Div_visitor_Pacific   0.04070987389743503\n",
      "18: Div_visitor_Southeast   3.35675874207771\n",
      "19: Div_visitor_Southwest   2.1236973143637985\n",
      "20: visitor_played_previous_date   7.162354670264624\n",
      "21: visitor_played_two_days_ago   3.072813891931474\n",
      "22: visitor_played_three_days_ago   3.1033347233655695\n",
      "23: visitor_played_prorrogue_previous_date   0.1331800271992788\n",
      "24: visitor_played_prorrogue_two_days_ago   0.23913738602210144\n",
      "25: visitor_played_prorrogue_three_days_ago   7.557930868684641\n"
     ]
    }
   ],
   "source": [
    "# scores del chi-square\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('%d: ' % i + str(chi2_variables[i]) +\"   \"+  str(fs.scores_[i]))\n",
    "# plot the scores\n",
    "pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local_played_prorrogue_previous_date\n",
      "Div_local_Atlantic\n",
      "Div_local_Northwest\n",
      "Conf_visitor_Este\n",
      "Div_visitor_Southeast\n",
      "Div_visitor_Southwest\n",
      "visitor_played_two_days_ago\n",
      "visitor_played_prorrogue_two_days_ago\n",
      "visitor_played_prorrogue_three_days_ago\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "fs = SelectKBest(score_func=mutual_info_classif, k='all', )\n",
    "fs.fit(x_train[dummy], y_train)\n",
    "x_train_fs = fs.transform(x_train[dummy])\n",
    "x_test_fs = fs.transform(x_test[dummy])\n",
    "\n",
    "mask = fs.get_support()\n",
    "info_variables = x_train[dummy].columns[mask]\n",
    "\n",
    "# Seleccionamos aquellas variables que tienen un score superior a la media (evaluar este criterio...)\n",
    "info_var = []\n",
    "for b in range(len(fs.scores_)):\n",
    "    if fs.scores_[b] > mean(fs.scores_):\n",
    "        info_var.append(info_variables[b])\n",
    "        print(info_variables[b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: local_played_previous_date   0.0\n",
      "1: local_played_two_days_ago   0.0023454303746102134\n",
      "2: local_played_three_days_ago   0.002121292231349825\n",
      "3: local_played_prorrogue_previous_date   0.009234971786677093\n",
      "4: local_played_prorrogue_two_days_ago   0.0015118802407265708\n",
      "5: local_played_prorrogue_three_days_ago   0.0\n",
      "6: Conf_local_Este   0.0\n",
      "7: Div_local_Atlantic   0.0036418702260241353\n",
      "8: Div_local_Central   0.0\n",
      "9: Div_local_Northwest   0.00933540285504586\n",
      "10: Div_local_Pacific   0.0\n",
      "11: Div_local_Southeast   0.0\n",
      "12: Div_local_Southwest   0.0\n",
      "13: Conf_visitor_Este   0.01147312411854462\n",
      "14: Div_visitor_Atlantic   0.00027651129274519626\n",
      "15: Div_visitor_Central   0.0\n",
      "16: Div_visitor_Northwest   0.0\n",
      "17: Div_visitor_Pacific   0.0\n",
      "18: Div_visitor_Southeast   0.0030788764230689214\n",
      "19: Div_visitor_Southwest   0.008333702081742\n",
      "20: visitor_played_previous_date   0.0\n",
      "21: visitor_played_two_days_ago   0.007754966345136838\n",
      "22: visitor_played_three_days_ago   0.0\n",
      "23: visitor_played_prorrogue_previous_date   0.0\n",
      "24: visitor_played_prorrogue_two_days_ago   0.010975090363924966\n",
      "25: visitor_played_prorrogue_three_days_ago   0.009226147620605696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 26 artists>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores del chi-square\n",
    "for i in range(len(fs.scores_)):\n",
    "    print('%d: ' % i + str(info_variables[i]) +\"   \"+  str(fs.scores_[i]))\n",
    "# plot the scores\n",
    "plt.bar([i for i in range(len(fs.scores_))], fs.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=U9h1xkNELvY\n",
    "\n",
    "https://machinelearningmastery.com/information-gain-and-mutual-information/\n",
    "\n",
    "https://blog.quantinsti.com/gini-index/\n",
    "\n",
    "Este resultados me gusta más (principalmente por la lógica de las variables).\n",
    "\n",
    "Sobre el test de independencia de chi-cuadrado me parece subjetivo los resultados, en gran parte porque el cálculo del test se basa en una matriz de confusion sobre la frecuencia de los resultados. Esto puede contribuir a relaciones de dependecia espúrea.\n",
    "\n",
    "En cuanto a la Ganancia de Información Mutua, es posible que sea mas preciso en estos casos porque sus calculos son utilizados en procesos de construccion de árboles de decision.\n",
    "\n",
    "La Información  Mutua (es lo mismo que Ganancia de la Informacion) mide la reducción de la incertidumbre para una variable dado un valor conocido de la otra variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisis de Matriz de Correlación\n",
    "\n",
    "### Importante:\n",
    "\n",
    "El descarte de variables por escasez de correlación o por multicolinealidad solo tiene sentido si se va a plantear un modelo sustentado en estimaciones lineales, de lo contrario la exclusión de estas contribuye al deteriodo del funcionamiento de otros modelos, por ejemplo, cualquiera derivado de \"árboles de decisión\".\n",
    "\n",
    "El principal argumento es que la medida de dependencia entre variables no necesariamente es lineal, y de ser lineal, los algoritmos basados en árboles de decisión dividen entre sus funciones lógicas de decisión las variables que son perfectamente correlacionadas.\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "Sobre la variable **target** descartamos las siguientes variables:\n",
    "\n",
    "**LOCAL_porcentajeVictorias** y escogemos LOCAL_porcentaje_victoria_LOCAL dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.86)\n",
    "\n",
    "**LOCAL_Posicion_division** y escogemos LOCAL_Posicion dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.83)\n",
    "\n",
    "**LOCAL_Posicion_conferencia** y escogemos LOCAL_Posicion dado que esta tiene mayor nivel de correlación con la variable Target, y no se escogen ambas porque estas estan muy correlacionadas entre si (.97)\n",
    "\n",
    "**Conf_local_west** y escogemos Conf_local_east por la inversa correlación perfecta que existe entre estas. No tiene sentido contemplar ambas ya que una contiene la información de la otra cuando toma valor cero.\n",
    "\n",
    "Si existe la posibilidad de descartar otra variable porque pueda generar ruido en el modelo sería **LOCAL_Racha** pero dejemos esta variable mientras tanto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partidos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables por Factor de Inflación de la Varianza (VIF)\n",
    "\n",
    "La existencia de multicolinealidad --explicación de variables independientes entre si mediante una combinación lineal de otras variables independientes-- es un problema grave en cualquier método estadístico que tenga como objetivo la predicción de una variable. Una de las técnicas que se puede utilizar para identificar este problema es el VIF.\n",
    "\n",
    "**El objetivo principal es eliminar aquellas variables que se explican entre si.**\n",
    "\n",
    "Vamos a calcular el valor del VIF para todas las variables menos la objetivo. Para esto se realiza una regresión lineal de cada una de las variables frente al resto y aplicamos la fórmula del VIF\n",
    "$$\n",
    "    VIF_i = \\frac{1}{1 - R_i^2}\n",
    "$$\n",
    "El valor del VIF se encuentra acotado ente 1 (no existe multicolinealidad) e infinito (existe una multicolinealidad perfecta). Es habitual eliminar las características con un valor por encima de 5, aunque dependiendo del número de características se puede relajar este criterio.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "Esta funcion no es mas que la iteración de una regresión lineal de cada variable respecto a otra.\n",
    "\n",
    "Al ser siempre una regresión simple, el coeficiente de determinacion (R-square) es simplemente el cuadrado del coeficiente de correlación de Pearson.\n",
    "\n",
    "En este contexto (para lo que nos interesa) mientras más cercano a 1 mejor, porque quiere decir que menos correlación tiene una variable respecto a la otra.\n",
    "\n",
    "\n",
    "Excluimos las variables que tengan VIF mayor a 5 porque son aquellas que tienen un R-square mayor a 0,8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def calculateVIF(data):\n",
    "    features = list(data.columns)\n",
    "    num_features = len(features)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    result = pd.DataFrame(index = ['VIF'], columns = features)\n",
    "    result = result.fillna(0)\n",
    "    \n",
    "    for ite in range(num_features):\n",
    "        x_features = features[:]\n",
    "        y_featue = features[ite]\n",
    "        x_features.remove(y_featue)\n",
    "        \n",
    "        x = data[x_features]\n",
    "        y = data[y_featue]\n",
    "        \n",
    "        model.fit(data[x_features], data[y_featue])\n",
    "        \n",
    "        if model.score(data[x_features], data[y_featue]) == 1:\n",
    "            result[y_featue] = Infinity\n",
    "        else:\n",
    "            result[y_featue] = 1/(1 - model.score(data[x_features], data[y_featue]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "calculateVIF(partidos.loc[:,partidos.columns != 'target']) # Excluimos la variale de estuido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como la funcion anterior da salida a cada una de las variables con su VIF correspondiente, ahora construimos una funcion para la exclusión de aquellas con VIF mayor a 5. \n",
    "\n",
    "**La salida de esta funcion es una lista de variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selectDataUsingVIF(data, max_VIF = 5):\n",
    "    result = data.copy(deep = True)\n",
    "    \n",
    "    VIF = calculateVIF(result)\n",
    "    \n",
    "    while VIF.values.max() > max_VIF:\n",
    "        col_max = np.where(VIF == VIF.values.max())[1][0]\n",
    "        features = list(result.columns)\n",
    "        features.remove(features[col_max])\n",
    "        print('Se ha eliminado: ----- '+ str(features[col_max]) + \" ----- VIF:  \" + \n",
    "              str(VIF[features[col_max]].values))\n",
    "        \n",
    "        result = result[features]\n",
    "        \n",
    "        VIF = calculateVIF(result)\n",
    "        \n",
    "    return result\n",
    "\n",
    "variables_vif = list(calculateVIF(selectDataUsingVIF(partidos.loc[:,partidos.columns != 'target'], 5)).columns)\n",
    "# Evaluar algun cambio en el código ya que se observa un error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El paso anterior esta pendiente de evaluación, solo se ha dejado para tener el código listo para ejecución\n",
    "\n",
    "\n",
    "\n",
    "## Ajustes de tipos de variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partidos['fecha'] = pd.to_datetime(partidos['fecha'].astype(str), format='%Y%m%d')\n",
    "#partidos.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORADA 2017-2018\n",
    "temporada_2017_2018 = data_1_clean[data_1_clean['fecha'] >= '2017-09-01']\n",
    "temporada_2017_2018 = temporada_2017_2018[temporada_2017_2018['fecha'] <= '2018-04-11'].sort_values('fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temporada_2017_2018 = temporada_2017_2018.reset_index(drop=True)\n",
    "\n",
    "\n",
    "temporada_2017_2018.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMPORADA 2018-2019\n",
    "temporada_2018_2019 = data_1_clean[data_1_clean['fecha'] >= '2018-09-01']\n",
    "temporada_2018_2019 = temporada_2018_2019[temporada_2018_2019['fecha'] <= '2019-04-10'].sort_values('fecha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temporada_2018_2019 = temporada_2018_2019.reset_index(drop=True)\n",
    "\n",
    "\n",
    "temporada_2018_2019.head(5)\n",
    "\n",
    "---------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte II: Selección de modelos.\n",
    "\n",
    "## Entrenamiento y Test inicial\n",
    "\n",
    "#### Logistic Regression\n",
    "#### Random Forest Classifier\n",
    "\n",
    "Antes de comenzar con la creación del modelo es necesario separar la muestra para el entrenamiento y test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el correcto funcionamiento de las funciones creadas en este apartado y que sea posible la comparación de modelos es estrictamente necesario que los datos de test y entrenamientos tengan el mismo tamaño.\n",
    "\n",
    "Sabemos que este no es el mejor enfoque, pero es el más sencillo y rápido que ayudará a determinar cuales son los modelos con mejor poder de predicción en entrenamiento y test.\n",
    "\n",
    "Es importante destacar que el **% de caida** del modelo es ajustable mediante tecnicas de preparacion de los datos, hiperparametrizacion de los modelos y otras tecnicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HAY QUE ELIMINAR LA SECUENCIA DE LOS DATOS POR FECHA\n",
    "\n",
    "\n",
    "# un train y test del mismo tamaño para hacer la primera labor de investigacion del modelo\n",
    "#variables = partidos.loc[:, partidos.columns != 'fecha'] \n",
    "\n",
    "\n",
    "#variables_2017_2018 = temporada_2017_2018.loc[:,temporada_2017_2018.columns != 'fecha']\n",
    "\n",
    "#variables_2018_2019 = temporada_2018_2019.loc[:,temporada_2018_2019.columns != 'fecha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# variables_2017_2018, target_2017_2018 = temporada_2017_2018.iloc[:, :-1], temporada_2017_2018.iloc[:, -1:]\n",
    "#variables_2018_2019, target_2018_2019 = temporada_2018_2019.iloc[:, :-1], temporada_2018_2019.iloc[:, -1:]\n",
    "\n",
    "variables = partidos[1:] # .copy si es par el len()\n",
    "\n",
    "variables, target = variables.iloc[:, :-1], variables.iloc[:, -1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size = 0.5, random_state=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2904, 98)\n",
      "(2904, 98)\n",
      "(2904, 1)\n",
      "(2904, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRESENCIA DE DATOS DESBALANCEADOS (¿?) PENDIENTE DE EVALUAR\n",
    "\n",
    "### # Esto ya no parece tener sentido YO ESTABA TRABAJANDO CON UN .CSV ANTIGUO........\n",
    "https://towardsdatascience.com/what-to-do-when-your-classification-dataset-is-imbalanced-6af031b12a36\n",
    "\n",
    "\n",
    "En teoria se deberia realizar una operacion de **Oversampling** ¿por que?:\n",
    "\n",
    "Cuando el conjunto de datos no representa todas las clases de datos por igual, el modelo podría ajustarse en exceso a la clase que está más representada en su conjunto de datos (en nuestro caso como es de esperarse mayor wins por parte del local) y pasar por alto la existencia de la clase minoritaria (los loses del local).\n",
    "\n",
    "En este contexto, de forma exagerada, imaginemos que nuestro modelo siempre estima win para local, dado los datos, sobre la totalidad de resultados igual a 1 tendrá un accuracy aprox. igual a 59%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    3389\n",
       "0    2420\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partidos.target.value_counts() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cómo evaluar correctamente un modelo desbalanceado:\n",
    "\n",
    "### Falta investigar\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall \n",
    "\n",
    "Hay que evaluar si se realiza el oversampling o no. De no realizarse, es importante las métricas de evaluación del modelo:\n",
    "\n",
    "#####  colocar formula con latex\n",
    "\n",
    "El **accuracy** puede ser una métrica engañosa para conjuntos de datos desequilibrados. Considere una muestra con 95 valores negativos y 5 positivos. La clasificación de todos los valores como negativos en este caso da un puntaje de precisión de 0.95. Hay muchas métricas que no sufren este problema. \n",
    "\n",
    "Por ejemplo, la precisión equilibrada (bACC) normaliza las predicciones verdaderas positivas y negativas verdaderas por el número de muestras positivas y negativas, respectivamente, y divide su suma entre dos:\n",
    "\n",
    "\n",
    "##### colocar formula con latex\n",
    "\n",
    "Para el ejemplo anterior (95 muestras negativas y 5 positivas), clasificarlas todas como negativas da un puntaje de precisión equilibrado de 0,5 (el puntaje máximo de bACC es uno), que es equivalente al valor esperado de una suposición aleatoria en un conjunto de datos equilibrado. La precisión equilibrada puede servir como una medida de rendimiento general para un modelo, ya sea que las etiquetas verdaderas estén o no desequilibradas en los datos, suponiendo que el costo de FN (falso negativo) sea el mismo que FP (falso positivo).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def GS(a,b):\n",
    "    \"\"\"\"\"\n",
    "    Función que recibe dos parámetros;\n",
    "    :a: una variable binaria que representa 0 = bueno y 1 = malo (objetivo)\n",
    "    :b: predicción de la primera variable (continua, entera o binaria)\n",
    "    :return: coeficiente GINI de las dos variables anteriores. \"\"\"\n",
    "    \n",
    "    gini = 2*roc_auc_score(a,b)-1\n",
    "    return gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcion de entranamiento de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_method(x_train, y_train, x_test, y_test, method):  \n",
    "    \"\"\"\n",
    "    Funcion para entrenar un modelo con el método seleccionado\n",
    "    El entrenamiento y algoritmos son de la libreria de sklearn.\n",
    "    :param x_train: numpy array, required\n",
    "    :param y_train: numpy array, required\n",
    "    :param x_test: numpy array, required\n",
    "    :param y_test: numpy array, required    \n",
    "    :return: object\n",
    "        - Modelo entrenado según los datos\n",
    "    \"\"\"    \n",
    "    if method == 'LR':  # Linear Regresssion\n",
    "        return LR(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'LOGR': # Logistic Regresssion\n",
    "        return LOGR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'DT': # Decision Tree Classifier\n",
    "        return DT(x_train, y_train, x_test, y_test)    \n",
    "    \n",
    "    elif method == 'LASSO': # Lasso Regresssion \n",
    "        return LASSO(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RIDGE': # Ridge Regresssion\n",
    "        return RIDGE(x_train, y_train, x_test, y_test)\n",
    "    \n",
    "    elif method == 'RFR': # Random Forest Regressor\n",
    "        return RFR(x_train, y_train, x_test, y_test)\n",
    "\n",
    "    elif method == 'RFC': # Random Forest Classifier\n",
    "        return RFC(x_train, y_train, x_test, y_test)    \n",
    "\n",
    "    elif method == 'GBR': # Gradient Boosting Regression\n",
    "        return GBR(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones resumen de algoritmos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def LR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Linear Regresssion\n",
    "    \"\"\"\n",
    "    model = LinearRegression().fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "def LOGR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Logistic Regresssion\n",
    "    \"\"\"\n",
    "    model = LogisticRegression().fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DT(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Decision Tree Classifier\n",
    "    \"\"\"\n",
    "    model = DecisionTreeClassifier(random_state=99).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "def LASSO(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Lasso Regresssion\n",
    "    \"\"\"\n",
    "    model = Lasso(alpha = 0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "def RIDGE(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Ridge Regresssion\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha = 0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "def RFR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Regressor\n",
    "    \"\"\"\n",
    "    model = RandomForestRegressor(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def RFC(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Random Forest Classifier\n",
    "    \"\"\"\n",
    "    model = RandomForestClassifier(n_estimators=1000, min_samples_split=2).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "def GBR(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Gradient Boosting Regression\n",
    "    \"\"\"\n",
    "    model = GradientBoostingRegressor(n_estimators=1000,alpha=0.01).fit(X_train, y_train)\n",
    "    return prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función de entrenamiento de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dictionary_of_measures(model, X_train, y_train, X_test, y_test):\n",
    "    # MEDIDAS DE PRUEBA\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad.\n",
    "        y_pred_train = model.predict_proba(X_train)[:,1] #seleccionamos solo la columna de prob. igual a 1\n",
    "    except:\n",
    "        y_pred_train = model.predict(X_test)\n",
    "        \n",
    "    a_train = model.score(X_train, y_train)\n",
    "    gini_train = GS(y_train,y_pred_train) # he modificado: tenia y_test y no y_train    \n",
    "\n",
    "    # MEDIDAS DE TEST\n",
    "    try: # Si es un método de clasificación, usamos la probabilidad.\n",
    "        y_pred_test = model.predict_proba(X_test)[:,1] #seleccionamos solo la columna de prob. igual a 1\n",
    "    except:\n",
    "        y_pred_test = model.predict(X_test) \n",
    "        \n",
    "    a_test = model.score(X_test, y_test)\n",
    "    gini_test = GS(y_test,y_pred_test)    \n",
    "\n",
    "    return {'model':model,'accuracy_train':a_train,'accuracy_test':a_test,\n",
    "            'gini_train':gini_train,'gini_test':gini_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
      "         normalize=False), 'accuracy_train': 0.5148617779007559, 'accuracy_test': -3349527.2810699204, 'gini_train': -0.011608885380347767, 'gini_test': 0.8284297439906196}\n"
     ]
    }
   ],
   "source": [
    "dict_trained_model = train_method(x_train, y_train,x_test,y_test,\"LR\")\n",
    "print(dict_trained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de comparación de modelos (Gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_list = ['LR','LASSO','LOGR','RIDGE', 'DT','RFR','RFC','GBR'] \n",
    "df_result_summary = pd.DataFrame(index=method_list,columns=['GINI-train','GINI-test', 'Caida %'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-a56cae231bf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mC3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmethod_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdict_trained_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Predicciones - REGRESSIONS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'LR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'LASSO'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RIDGE'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RFR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'GBR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-a3a1008e0ac6>\u001b[0m in \u001b[0;36mtrain_method\u001b[1;34m(x_train, y_train, x_test, y_test, method)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RFR'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Random Forest Regressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mRFR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'RFC'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Random Forest Classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-c22ec748c6ca>\u001b[0m in \u001b[0;36mRFR\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mRandom\u001b[0m \u001b[0mForest\u001b[0m \u001b[0mRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \"\"\"\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprepare_dictionary_of_measures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    331\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 333\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 920\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    921\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    922\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    714\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    223\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 225\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    117\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1142\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1143\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "C1 = []\n",
    "C2 = []\n",
    "C3 = []\n",
    "for method in method_list:\n",
    "    dict_trained_model[method] = train_method(x_train, y_train,x_test,y_test,method)\n",
    "    # Predicciones - REGRESSIONS\n",
    "    if method in ['LR','LASSO','RIDGE','RFR','GBR']: \n",
    "        y_pred_train = dict_trained_model[method]['model'].predict(x_train)\n",
    "        y_pred_test = dict_trained_model[method]['model'].predict(x_test)\n",
    "    else: # Predicciones - CLASSIFIERS \n",
    "        y_pred_train = dict_trained_model[method]['model'].predict_proba(x_train)[:,1]\n",
    "        y_pred_test = dict_trained_model[method]['model'].predict_proba(x_test)[:,1]\n",
    "    # Calculo de GINI\n",
    "    gini_score_train = GS(y_train,y_pred_train)\n",
    "    C1.append(gini_score_train)\n",
    "    gini_score_test = GS(y_test,y_pred_test)\n",
    "    C2.append(gini_score_test)\n",
    "    \n",
    "    diferencia = (1-(gini_score_test/gini_score_train)) * 100\n",
    "    C3.append(diferencia)\n",
    "    \n",
    "    \n",
    "# Adding the results to: df_result_summary\n",
    "df_result_summary ['GINI-train'] = C1\n",
    "df_result_summary ['GINI-test'] = C2\n",
    "df_result_summary ['Caida %'] = C3\n",
    "\n",
    "# Printing out the summary table\n",
    "df_result_summary.sort_values('GINI-test', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe de comparación de modelos (Accuracy)\n",
    "\n",
    "#### Accuracy (exactitud)\n",
    "La exactitud es una métrica para evaluar modelos de resultados de clasificación. \n",
    "\n",
    "Informalmente, la exactitud es la fracción de predicciones que el modelo realizó correctamente.\n",
    "\n",
    "Formalmente, la exactitud tiene la siguiente definición:\n",
    "\n",
    "##### Colocar formula con Latex\n",
    "\n",
    "Donde VP = Verdaderos positivos, VN = Verdaderos negativos, FP = Falsos positivos y FN = Falsos negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for method in method_list:\n",
    "    dict_trained_model[method] = train_method(x_train, y_train,x_test,y_test,method)\n",
    "#    print (dict_trained_model[method])\n",
    "    data.append([method, dict_trained_model[method]['accuracy_train'], dict_trained_model[method]['accuracy_test']])\n",
    "\n",
    "df_result_summary_2 = pd.DataFrame(data,columns=['method','accuracy_train','accuracy_test']) \n",
    "                \n",
    "df_result_summary_2.sort_values('accuracy_test', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anexo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_list = \"\"\"{\"svm_linear\": SVC(probability=True, kernel='linear', C=1.0),\n",
    "                       \"svm_poly\": SVC(probability=True, kernel='poly', C=1.0),\n",
    "                       \"svm_rbf\": SVC(probability=True, kernel='rbf', C=1.0, gamma=0.01),\n",
    "                       \"linear_svc\": LinearSVC(penalty='l2', loss='squared_hinge', dual=True, tol=0.1, C=1.0, multi_class='ovr', fit_intercept=True,\n",
    "                                               intercept_scaling=1, random_state=None, max_iter=3000),\n",
    "                       \"knn\": KNeighborsClassifier(n_neighbors=100, weights='distance', leaf_size=30, n_jobs=n_jobs),\n",
    "                       \"random_forests\": RandomForestClassifier(n_estimators=350, criterion='entropy', min_samples_split=2,\n",
    "                                                                min_samples_leaf=1, max_leaf_nodes=600, n_jobs=n_jobs),\n",
    "                       \"logistic_regression\": LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=2.4, fit_intercept=True, intercept_scaling=1,\n",
    "                                                                 random_state=None, solver='liblinear', max_iter=1000, multi_class='ovr',\n",
    "                                                                 warm_start=False, n_jobs=n_jobs),\n",
    "                       \"decision_trees\": DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None, min_samples_split=2,\n",
    "                                                                min_samples_leaf=100, min_weight_fraction_leaf=0.0, max_features=None,\n",
    "                                                                random_state=None, max_leaf_nodes=None, presort=False),\n",
    "                       \"sgd\": SGDClassifier(alpha=.0001, n_iter=500, penalty=\"elasticnet\", n_jobs=n_jobs),\n",
    "                       \"neural_network\": Classifier(layers=[Layer(\"Sigmoid\", units=14), Layer(\"Sigmoid\", units=13), Layer(\"Sigmoid\", units=12),\n",
    "                                                            Layer(\"Sigmoid\", units=10), Layer(\"Softmax\")], learning_rate=0.01, n_iter=200,\n",
    "                                                    batch_size=10, regularize='L1', n_stable=50, dropout_rate=0, verbose=True),\n",
    "                       \"GBC\": GradientBoostingClassifier(max_depth=10, max_leaf_nodes=850, min_samples_leaf=15, learning_rate=0.1),\n",
    "                       \"XGB\": XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
    "                                            max_depth=10, min_child_weight=2, missing=None, n_estimators=100, nthread=n_jobs, reg_alpha=0,\n",
    "                                            objective='binary:logistic', reg_lambda=1, scale_pos_weight=1, seed=0, silent=True, subsample=1)}\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables con RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE IMPORTANCE Random Forest Classifier\n",
      "\n",
      "1. visitor_Conf_position (0.091066) \n",
      "2. VISITANTE_Ultimos10Victorias (0.053762) \n",
      "3. local_Conf_position (0.046102) \n",
      "4. local_played_prorrogue_three_days_ago (0.031168) \n",
      "5. local_fg3 (0.030794) \n",
      "6. local_Home_lose (0.030723) \n",
      "7. local_ast (0.029453) \n",
      "8. Div_visitor_Atlantic (0.026450) \n",
      "9. visitor_Home_win (0.025862) \n",
      "10. avg_distance_place_visitor_played (0.021184) \n",
      "11. local_fta (0.020796) \n",
      "12. visitor_fg3 (0.018962) \n",
      "13. local_fg_pct (0.017520) \n",
      "14. local_played_prorrogue_two_days_ago (0.016493) \n",
      "15. visitor_pts (0.016461) \n",
      "16. LOCAL_porcentaje_victoria_LOCAL (0.015609) \n",
      "17. local_stl (0.015162) \n",
      "18. local_played_three_days_ago (0.013320) \n",
      "19. LOCAL_porcentaje_victoria_Ultimos10 (0.012797) \n",
      "20. Div_local_Central (0.012256) \n",
      "21. local_played_previous_date (0.011720) \n",
      "22. Div_local_Atlantic (0.011353) \n",
      "23. visitor_pf (0.011204) \n",
      "24. local_played_prorrogue_previous_date (0.011118) \n",
      "25. local_ft (0.010951) \n",
      "26. local_blk (0.010859) \n",
      "27. visitor_played_two_days_ago (0.010783) \n",
      "28. Div_visitor_Central (0.010448) \n",
      "29. visitor_played_prorrogue_three_days_ago (0.010332) \n",
      "30. visitor_dif_between_previous_game (0.010224) \n",
      "31. Div_local_Northwest (0.010111) \n",
      "32. Sueldo visitante (0.010029) \n",
      "33. Div_visitor_Northwest (0.009697) \n",
      "34. local_Percentagewl (0.009641) \n",
      "35. local_pf (0.009599) \n",
      "36. local_tov (0.009576) \n",
      "37. local_visitor_dif_pts (0.009562) \n",
      "38. Conf_visitor_Este (0.009391) \n",
      "39. Conf_local_Este (0.009320) \n",
      "40. Div_visitor_Southwest (0.009311) \n",
      "41. visitor_Home_lose (0.009311) \n",
      "42. visitor_fta (0.009082) \n",
      "43. visitor_fg_pct (0.009016) \n",
      "44. local_fga (0.008988) \n",
      "45. Div_visitor_Pacific (0.008885) \n",
      "46. Div_local_Southwest (0.008871) \n",
      "47. visitor_fga (0.008468) \n",
      "48. visitor_played_local (0.008314) \n",
      "49. avg_distance_place_local_played (0.008261) \n",
      "50. Sueldo local (0.008129) \n",
      "51. local_fg3_pct (0.008113) \n",
      "52. LOCAL_Ultimos10Derrotas (0.008035) \n",
      "53. visitor_played_visitor (0.007839) \n",
      "54. visitor_fg3_pct (0.007342) \n",
      "55. distance_between_stadiums (0.007234) \n",
      "56. visitor_drb (0.007182) \n",
      "57. local_ft_pct (0.006993) \n",
      "58. VISITOR_AWS_MEDIO_AGRUPADO (0.006986) \n",
      "59. visitor_tov (0.006935) \n",
      "60. distance_local_traveled (0.006874) \n",
      "61. LOCAL_Ultimos10Victorias (0.006768) \n",
      "62. visitor_trb (0.006633) \n",
      "63. local_fg3a (0.006255) \n",
      "64. VISITANTE_Ultimos10Derrotas (0.006210) \n",
      "65. visitor_played_previous_date (0.006063) \n",
      "66. visitor_played_three_days_ago (0.005866) \n",
      "67. visitor_Dif_leader (0.005015) \n",
      "68. VISITANTE_porcentaje_victoria_VISITANTE (0.004211) \n",
      "69. visitor_Percentagewl (0.003766) \n",
      "70. visitor_played_prorrogue_previous_date (0.003458) \n",
      "71. visitor_orb (0.002664) \n",
      "72. visitor_played_prorrogue_two_days_ago (0.001917) \n",
      "73. Div_local_Southeast (0.001903) \n",
      "74. visitor_blk (0.001891) \n",
      "75. Div_local_Pacific (0.001813) \n",
      "76. VISITANTE_porcentaje_victoria_Ultimos10 (0.001670) \n",
      "77. local_orb (0.001629) \n",
      "78. LOCAL_Racha (0.001610) \n",
      "79. local_played_visitor (0.001582) \n",
      "80. local_Dif_leader (0.001489) \n",
      "81. visitor_ft (0.001489) \n",
      "82. local_drb (0.001416) \n",
      "83. local_dif_between_previous_game (0.001336) \n",
      "84. local_Home_win (0.001261) \n",
      "85. Div_visitor_Southeast (0.001236) \n",
      "86. visitor_ft_pct (0.001223) \n",
      "87. visitor_stl (0.001184) \n",
      "88. visitor_fg (0.001046) \n",
      "89. VISITANTE_Racha (0.000728) \n",
      "90. distance_visitor_traveled (0.000723) \n",
      "91. visitor_fg3a (0.000700) \n",
      "92. visitor_ast (0.000588) \n",
      "93. local_played_two_days_ago (0.000581) \n",
      "94. local_played_local (0.000560) \n",
      "95. LOCAL_AWS_MEDIO_AGRUPADO (0.000495) \n",
      "96. local_pts (0.000440) \n",
      "97. local_fg (0.000341) \n",
      "98. local_trb (0.000213) \n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "list_var = list(set(variables.columns))\n",
    "\n",
    "print (\"FEATURE IMPORTANCE Random Forest Classifier\")\n",
    "print(\"\")\n",
    "\n",
    "       \n",
    "forest = RandomForestClassifier()\n",
    "\n",
    "forest.fit(variables, target) # Este paso es importante: la selección es respecto a la totalidad de los datos\n",
    "\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Ranking\n",
    "for f in range(len(variables.columns)):\n",
    "    print(\"%d. %s (%f) \" % (f + 1, list_var[indices[f]], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['visitor_Conf_position',\n",
       " 'VISITANTE_Ultimos10Victorias',\n",
       " 'local_Conf_position',\n",
       " 'local_played_prorrogue_three_days_ago',\n",
       " 'local_fg3',\n",
       " 'local_Home_lose',\n",
       " 'local_ast',\n",
       " 'Div_visitor_Atlantic',\n",
       " 'visitor_Home_win',\n",
       " 'avg_distance_place_visitor_played',\n",
       " 'local_fta',\n",
       " 'visitor_fg3',\n",
       " 'local_fg_pct',\n",
       " 'local_played_prorrogue_two_days_ago',\n",
       " 'visitor_pts',\n",
       " 'LOCAL_porcentaje_victoria_LOCAL',\n",
       " 'local_stl',\n",
       " 'local_played_three_days_ago',\n",
       " 'LOCAL_porcentaje_victoria_Ultimos10',\n",
       " 'Div_local_Central',\n",
       " 'local_played_previous_date',\n",
       " 'Div_local_Atlantic',\n",
       " 'visitor_pf',\n",
       " 'local_played_prorrogue_previous_date',\n",
       " 'local_ft',\n",
       " 'local_blk',\n",
       " 'visitor_played_two_days_ago',\n",
       " 'Div_visitor_Central',\n",
       " 'visitor_played_prorrogue_three_days_ago',\n",
       " 'visitor_dif_between_previous_game',\n",
       " 'Div_local_Northwest',\n",
       " 'Sueldo visitante',\n",
       " 'Div_visitor_Northwest',\n",
       " 'local_Percentagewl',\n",
       " 'local_pf',\n",
       " 'local_tov',\n",
       " 'local_visitor_dif_pts',\n",
       " 'Conf_visitor_Este',\n",
       " 'Conf_local_Este',\n",
       " 'Div_visitor_Southwest',\n",
       " 'visitor_Home_lose',\n",
       " 'visitor_fta',\n",
       " 'visitor_fg_pct',\n",
       " 'local_fga',\n",
       " 'Div_visitor_Pacific',\n",
       " 'Div_local_Southwest',\n",
       " 'visitor_fga',\n",
       " 'visitor_played_local',\n",
       " 'avg_distance_place_local_played',\n",
       " 'Sueldo local']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_variables = []\n",
    "for f in range(50): # eleccion arbitraria por mi, esto luego se va probando hasta determinar el optimo...\n",
    "    rf_variables.append(list_var[indices[f]])\n",
    "rf_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selección de Variables con Algoritmo Genético (GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "from deap import creator, base, tools, algorithms #GENETIC ALGORITHM LIBRARY - requirement: pip install deap\n",
    "import random\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"GENETIC ALGORITHM FOR FEATURE SELECTION:\")\n",
    "\n",
    "list_inputs = set(variables.columns)\n",
    "\n",
    "#####\n",
    "#SETING UP THE GENETIC ALGORITHM and CALCULATING STARTING POOL (STARTING CANDIDATE POPULATION)\n",
    "#####\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"attr_bool\", random.randint, 0, 1)\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_bool, n=len(list_inputs))\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "def evalOneMax(individual):\n",
    "    return sum(individual),\n",
    "\n",
    "toolbox.register(\"evaluate\", evalOneMax)\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutFlipBit, indpb=0.05)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=3)\n",
    "\n",
    "NPOPSIZE = len(variables.columns) #RANDOM STARTING POOL SIZE\n",
    "population = toolbox.population(n=NPOPSIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "#####\n",
    "#ASSESSING GINI ON THE STARTING POOL\n",
    "#####\n",
    "dic_gini={}\n",
    "\n",
    "for i in range(np.shape(population)[0]): \n",
    "\n",
    "    # TRASLATING DNA INTO LIST OF VARIABLES (1-81)\n",
    "    var_model = []    \n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if (population[i])[j]==1:\n",
    "            var_model.append(list(list_inputs)[j])\n",
    "\n",
    "    # ASSESSING GINI INDEX FOR EACH INVIVIDUAL IN THE INITIAL POOL \n",
    "\n",
    "    #X_train = partidos[var_model]\n",
    "    #Y_train = partidos[\"target\"]\n",
    "    \n",
    "    X_train = variables.copy()\n",
    "    Y_train = target.copy()\n",
    "\n",
    "    ######\n",
    "    # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "    #####      \n",
    "\n",
    "    lr = RandomForestClassifier()\n",
    "    model = lr.fit(X_train, Y_train)\n",
    "    Y_predict = model.predict_proba(X_train)[:,1]\n",
    "    \n",
    "    #yo_pred = result.predict_proba(Xo_std)[:,1] #este es para tener la prob. cuando usamos skit.learn    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #lr = sm.Logit(Y_train, X_train)\n",
    "    #model=lr.fit()   \n",
    "    #Y_predict=model.predict(X_train)\n",
    "    ######\n",
    "    # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "    #####             \n",
    "\n",
    "\n",
    "    ######\n",
    "    # CHANGE_HERE - START: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "    #####                \n",
    "    fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    gini_power= 2*roc_auc_score(Y_train, Y_predict)-1\n",
    "    #gini_power = abs(2*auc-1)\n",
    "    ######\n",
    "    # CHANGE_HERE - END: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "    #####                \n",
    "    \n",
    "    gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "    dic_gini[gini]=population[j] \n",
    "    \n",
    "list_gini=sorted(dic_gini.keys(),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "#GENETIC ALGORITHM MAIN LOOP - START\n",
    "# - ITERATING MANY TIMES UNTIL NO IMPROVMENT HAPPENS IN ORDER TO FIND THE OPTIMAL SET OF CHARACTERISTICS (VARIABLES)\n",
    "#####\n",
    "sum_current_gini=0.0\n",
    "sum_current_gini_1=0.0\n",
    "sum_current_gini_2=0.0\n",
    "first=0    \n",
    "OK = 1\n",
    "a=0\n",
    "while OK:  #REPEAT UNTIL IT DO NOT IMPROVE, AT LEAST A LITLE, THE GINI IN 2 GENERATIONS\n",
    "    a=a+1\n",
    "    print ('loop ', a)\n",
    "    OK=0\n",
    "\n",
    "    ####\n",
    "    # GENERATING OFFSPRING - START\n",
    "    ####\n",
    "    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1) #CROSS-X PROBABILITY = 50%, MUTATION PROBABILITY=10%\n",
    "    fits = toolbox.map(toolbox.evaluate, offspring)\n",
    "    for fit, ind in zip(fits, offspring):\n",
    "        ind.fitness.values = fit\n",
    "    population =toolbox.select(offspring, k=len(population))\n",
    "    ####\n",
    "    # GENERATING OFFSPRING - END\n",
    "    ####\n",
    "\n",
    "    sum_current_gini_2=sum_current_gini_1\n",
    "    sum_current_gini_1=sum_current_gini\n",
    "    sum_current_gini=0.0\n",
    "\n",
    "    #####\n",
    "    #ASSESSING GINI ON THE OFFSPRING - START\n",
    "    #####\n",
    "    for j in range(np.shape(population)[0]): \n",
    "        if population[j] not in dic_gini.values(): \n",
    "            var_model = [] \n",
    "            for i in range(np.shape(population)[0]): \n",
    "                if (population[j])[i]==1:\n",
    "                    var_model.append(list(list_inputs)[i])\n",
    "            \n",
    "            X_train= variables.copy()\n",
    "            Y_train= target.copy()\n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - START: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "            #####    \n",
    "    \n",
    "            \n",
    "            lr = RandomForestClassifier()\n",
    "            model = lr.fit(X_train, Y_train)\n",
    "            Y_predict = model.predict_proba(X_train)[:,1]\n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - END: YOU ARE VERY LIKELY USING A DIFFERENT TECHNIQUE BY NOW. SO CHANGE TO YOURS.\n",
    "            #####            \n",
    "                       \n",
    "            \n",
    "            ######\n",
    "            # CHANGE_HERE - START: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "            #####                       \n",
    "            fpr, tpr, thresholds = metrics.roc_curve(Y_train, Y_predict)\n",
    "            auc = metrics.auc(fpr, tpr)\n",
    "            gini_power= 2*roc_auc_score(Y_train, Y_predict)-1   \n",
    "            #gini_power = abs(2*auc-1)\n",
    "            ######\n",
    "            # CHANGE_HERE - END: HERE IT USES THE DEVELOPMENT GINI TO SELECT VARIABLES, YOU SHOULD A DIFFERENT GINI. EITHER THE OOT GINI OR THE SQRT(DEV_GINI*OOT_GINI)\n",
    "            #####                       \n",
    "           \n",
    "            gini=str(gini_power)+\";\"+str(population[j]).replace('[','').replace(', ','').replace(']','')\n",
    "            dic_gini[gini]=population[j]  \n",
    "    #####\n",
    "    #ASSESSING GINI ON THE OFFSPRING - END\n",
    "    #####\n",
    "\n",
    "    #####\n",
    "    #SELECTING THE BEST FITTED AMONG ALL EVER CREATED POPULATION AND CURRENT OFFSPRING - START\n",
    "    #####           \n",
    "    list_gini=sorted(dic_gini.keys(),reverse=True)\n",
    "    population=[]\n",
    "    for i in list_gini[:NPOPSIZE]:\n",
    "        population.append(dic_gini[i])\n",
    "        gini=float(i.split(';')[0])\n",
    "        sum_current_gini+=gini\n",
    "    #####\n",
    "    #SELECTING THE BEST FITTED AMONG ALL EVER CREATED POPULATION AND CURRENT OFFSPRING - END\n",
    "    #####           \n",
    "      \n",
    "    #HAS IT IMPROVED AT LEAST A LITLE THE GINI IN THE LAST 2 GENERATIONS\n",
    "    print ('sum_current_gini=', sum_current_gini, 'sum_current_gini_1=', sum_current_gini_1, 'sum_current_gini_2=', sum_current_gini_2)\n",
    "    if(sum_current_gini>sum_current_gini_1+0.0001 or sum_current_gini>sum_current_gini_2+0.0001):\n",
    "        OK=1\n",
    "#####\n",
    "#GENETIC ALGORITHM MAIN LOOP - END\n",
    "#####\n",
    "\n",
    "\n",
    "gini_max=list_gini[0]        \n",
    "gini=float(gini_max.split(';')[0])\n",
    "features=gini_max.split(';')[1]\n",
    "\n",
    "\n",
    "####\n",
    "# PRINTING OUT THE LIST OF FEATURES\n",
    "#####\n",
    "adn_variables = []\n",
    "f=0\n",
    "for i in range(len(features)):\n",
    "    if features[i]=='1':\n",
    "        f+=1\n",
    "        adn_variables.append(list(list_inputs)[i])\n",
    "        print ('feature ', f, ':', list(list_inputs)[i])\n",
    "print ('gini: ', gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametrización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def Grid_Search_CV_RFR(X_train, y_train):\n",
    "\n",
    "\n",
    "    estimator = RandomForestClassifier()\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 500, 1000],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2,4,8,16],\n",
    "        \"bootstrap\": [True, False],\n",
    "        \"criterion\": ['gini'] # se necesitan diversas pruebas, se que default es gini, probar entropy\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(estimator, param_grid, n_jobs=-1, cv=5)\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    return grid.best_score_ , grid.best_params_\n",
    "\n",
    "def RFC(X_train, X_test, y_train, y_test, best_params):\n",
    "\n",
    "    estimator = RandomForestClassifier(n_jobs=-1).set_params(**best_params)\n",
    "    estimator.fit(X_train,y_train)\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    print (\"GINI:\",GS(y_test,y_predict))\n",
    "    return y_test,y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Dejo comentado este codigo porque lleva mucho tiempo de ejecucion\n",
    "\n",
    "for i in range(5):\n",
    "    print (\"Loop: \" , i)\n",
    "    print (\"--------------\")\n",
    "    # Es importante dividir el set de datos en partes iguales 50% train y 50% test solo para evaluar\n",
    "    \n",
    "    best_score, best_params = Grid_Search_CV_RFR(x_train, y_train)\n",
    "    y_test , y_predict = RFC(x_train, x_test, y_train, y_test, best_params)\n",
    "    print (\"Best Score:\" ,best_score)\n",
    "    print (\"Best params:\",best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del loop anterior, para no volver a ejecutar que consume mucho tiempo:\n",
    "    \n",
    "\n",
    "Loop:  0\n",
    "--------------\n",
    "GINI: 0.4780002348265111\n",
    "Best Score: 0.7402501839587933\n",
    "Best params: {'bootstrap': False, 'criterion': 'gini', 'max_features': 'log2', 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Loop:  1\n",
    "--------------\n",
    "GINI: 0.49720817370203996\n",
    "Best Score: 0.739514348785872\n",
    "Best params: {'bootstrap': False, 'criterion': 'gini', 'max_features': 'auto', 'min_samples_split': 8, 'n_estimators': 100}\n",
    "Loop:  2\n",
    "--------------\n",
    "GINI: 0.4908352431106684\n",
    "Best Score: 0.7402501839587933\n",
    "Best params: {'bootstrap': False, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Loop:  3\n",
    "--------------\n",
    "GINI: 0.49748431228447054\n",
    "Best Score: 0.739514348785872\n",
    "Best params: {'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_split': 2, 'n_estimators': 500}\n",
    "Loop:  4\n",
    "--------------\n",
    "GINI: 0.5102279991476666\n",
    "Best Score: 0.7387785136129507\n",
    "Best params: {'bootstrap': True, 'criterion': 'gini', 'max_features': 'sqrt', 'min_samples_split': 8, 'n_estimators': 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte III: Evaluación del modelo\n",
    "\n",
    "## Enfoque sencillo de entrenamiento y test.\n",
    "\n",
    "### Modelos a evaluar:\n",
    "\n",
    "###### Random Forest Classifier\n",
    "###### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "variables = partidos[rf_variables]\n",
    "\n",
    "target = partidos.iloc[:, -1:]\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(variables, target, test_size = 0.30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4066, 50)\n",
      "(1743, 50)\n",
      "(4066, 1)\n",
      "(1743, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRENAMIENTO: Accuracy RFC, Accuracy LR:\n",
      "0.7584624211130235\n",
      "0.5857716580608147\n"
     ]
    }
   ],
   "source": [
    "model_list = {'RFC': RandomForestClassifier(),\n",
    "              'LR': LogisticRegression()}\n",
    "\n",
    "print(\"ENTRENAMIENTO: Accuracy RFC, Accuracy LR:\")\n",
    "for mod in['RFC', 'LR']:\n",
    "    result = model_list[mod].fit(x_train, y_train)\n",
    "    y_pred = result.predict_proba(x_train)[:,1]\n",
    "    yo_pred = result.predict_proba(x_test)[:,1] #este es para tener la prob. cuando usamos skit.learn\n",
    "    print(result.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posteriormente a la elección del mejor modelo, creamos un dataframe de comparación entre y_test y y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe de resultados\n",
    "data_result = x_test.copy()\n",
    "data_result['target'] = y_test['target']\n",
    "data_result['pred'] = yo_pred\n",
    "\n",
    "\n",
    "# Columna de predicciones\n",
    "data_result['target_pred'] = 0\n",
    "wins = data_result['pred'] >= 0.50\n",
    "column = 'target_pred'\n",
    "data_result.loc[wins, column] =  1\n",
    "\n",
    "#exitos = len(data_result[data_result['target'] == data_result['target_pred']])\n",
    "#fallos = len(data_result[data_result['target'] != data_result['target_pred']])\n",
    "#exito / (exito + fallos)\n",
    "data_result.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando Validación Cruzada\n",
    "\n",
    "Una desventaja de usar un conjunto de datos de test y otro de entrenamiento para la validación del modelo es que hemos perdido una parte importante de nuestros datos (entre el 20% y 30%) en la capacitación del modelo. Esto no es óptimo desde muchos puntos de vista, principalmente por **uso inadecuado de los recursos escasos (datos)**.\n",
    "\n",
    "¿Pocos datos? Si, para el caso de la NBA es que existen muchos años de datos, sin embargo, a priori se tienen hipótesis de cambios estructurales (forma del juego) que de alguna forma el modelo no podrá diferenciar y es posible que no tenga la capacidad de generalizar sobre datos no observados.\n",
    "\n",
    "En este contexto, lo mejor que podemos hacer es aprovechar al máximo los datos que tenemos. Para resolver este problema utilizaremos la validación cruzada.\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "**NOTA:** como parte del proyecto será importante plantear la hipótesis sobre el posible cambio estructural de los datos dado el cambio de juego (posiblemente un juego mas rapido, mas tiros de triples, mas puntos anotados, etc).\n",
    "\n",
    "Esta hipótesis sobre la generealizacion del modelo se puede abordar realizando un modelo con menos datos y otro con mas datos para posteriormente comparar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos la función `cross_val_score`. Esta función divide el conjunto de datos en diferentes muestras y entrena con todas menos una, dejando la restante para validación. Este proceso se repite según el número en el que se divida las muestras para la validación.\n",
    "\n",
    "Otro problema de utilizar esta técnica es saber **cuál es el número óptimo** de divisiones de la muestra.\n",
    "\n",
    "Estas divisiones de muestra se llaman **folds** determinada por la letra **\"k\"**. Dentro de los parámetros de `cross_val_score` se usan dos métodos para la division de la muestra,  KFold y StratifiedKFold.\n",
    "\n",
    "Por ahora utilizaremos como base el enfoque más común en problemas de Machine Learning determinando k= 10 sabiendo que este parámetro bien puede ir desde **k= 2** hasta **k = n** donde \"n\" el tamaño de los datos.\n",
    "\n",
    "\n",
    "Mediante `cv` se indica el numero de partes en las que se dividen los datos. La función devuelve un vector con el score de cada uno de los modelos construidos.\n",
    "\n",
    "https://machinelearningmastery.com/k-fold-cross-validation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector de Accuracy para cada CV del modelo de ENTRENAMIENTO\n",
      "\n",
      "[0.76044226 0.76752768 0.73923739 0.77244772 0.78105781]\n",
      "\n",
      "Vector de Accuracy de media y desviación típica de ENTRENAMIENTO\n",
      "\n",
      "0.7641425726296575 0.014145812495931564\n",
      "\n",
      "-------------------------------------------------------------------\n",
      "Vector de Accuracy para cada CV del modelo de ENTRENAMIENTO\n",
      "\n",
      "[0.62285012 0.58917589 0.61254613 0.62238622 0.6199262 ]\n",
      "\n",
      "Vector de Accuracy de media y desviación típica de ENTRENAMIENTO\n",
      "\n",
      "0.6133769126389053 0.012650550775869776\n",
      "\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iteracion entre modelos para comparación\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "for mod in['RFC', 'LR']:\n",
    "    scores = cross_val_score(model_list[mod], x_train, y_train, cv = 5) #cv variacion cruzada 10 modelos\n",
    "    print(\"Vector de Accuracy para cada CV del modelo de ENTRENAMIENTO\")\n",
    "    print(\"\")\n",
    "    print(scores)\n",
    "    print(\"\")\n",
    "    print(\"Vector de Accuracy de media y desviación típica de ENTRENAMIENTO\")\n",
    "    print(\"\")   \n",
    "    print(np.mean(scores), np.std(scores))\n",
    "    print(\"\")\n",
    "    print(\"-------------------------------------------------------------------\") \n",
    "# Primer output RF\n",
    "# Segundo output LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTA: \n",
    "\n",
    "Si existe un mal desempeño del modelo en el resultado anterior, se debe evaluar métodos que limiten la caida del accuracy o cualquier otra métrica de evaluación en los datos de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy del modelo en TEST usando Validación Cruzada seleccinando el mejor de k = 5\n",
      "[0.73352436 0.74498567 0.77077364 0.75644699 0.70893372]\n",
      "-------------------------------------------------------------------\n",
      "Accuracy del modelo en TEST usando Validación Cruzada seleccinando el mejor de k = 5\n",
      "[0.58452722 0.60458453 0.59598854 0.57020057 0.58501441]\n",
      "-------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Iteracion entre modelos para comparación\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "for mod in['RFC', 'LR']:\n",
    "    scores = cross_val_score(model_list[mod], x_test, y_test, cv = 5) # output del mejor modelo\n",
    "    print(\"Accuracy del modelo en TEST usando Validación Cruzada seleccinando el mejor de k = 5\")\n",
    "    print(scores)\n",
    "    print(\"-------------------------------------------------------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir un proceso para almacenar ambos resultados (entrenamiento y test) en dataframes (mejor visualizacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Código para guardar el modelo en su posterior uso\n",
    "\n",
    "\n",
    "#filename = 'nba_pred_model_RFC_basic_1.sav'\n",
    "#pickle.dump(nombre_del_objeto_modelo, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
